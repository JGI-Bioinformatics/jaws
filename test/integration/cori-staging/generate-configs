#!/usr/bin/env bash

set -eo pipefail

readonly configdir=${1:-/tmp/}

[[ -z $JAWS_SITE_CORI_DB_PW ]]     && { printf "Missing JAWS_SITE_CORI_DB_PW variable.\n"; exit 1; }
[[ -z $JAWS_SITE_CORI_RMQ_PW ]]    && { printf "Missing JAWS_SITE_CORI_RMQ_PW variable.\n"; exit 1; }
[[ -z $JAWS_CENTRAL_CORI_DB_PW ]]  && { printf "Missing JAWS_CENTRAL_CORI_DB_PW variable.\n"; exit 1; }
[[ -z $JAWS_CENTRAL_CORI_RMQ_PW ]] && { printf "Missing JAWS_CENTRAL_CORI_RMQ_PW variable.\n"; exit 1; }
[[ -z $JAWS_JTM_CORI_DB_PW ]]      && { printf "Missing JAWS_JTM_CORI_DB_PW variable.\n"; exit 1; }
[[ -z $JAWS_CROMWELL_CORI_DB_PW ]] && { printf "Missing JAWS_CROMWELL_CORI_DB_PW variable.\n"; exit 1; }


cat << EOM > $configdir/jaws-site.conf
[DB]
dialect = mysql+mysqlconnector
host = db.mysql.prod-cattle.stable.spin.nersc.org
port = 60006
user = jaws
password = $JAWS_SITE_CORI_DB_PW
db = jaws_staging
[LOCAL_RPC_SERVER]
user = jaws_nersc
password = $JAWS_SITE_CORI_RMQ_PW
host = rmq.lbl.gov
port = 5672
vhost = jaws_staging
queue = site_rpc
num_threads = 5
max_retries = 3
[CENTRAL_RPC_SERVER]
user = jaws_nersc
password = $JAWS_SITE_CORI_RMQ_PW
host = rmq.lbl.gov
port = 5672
vhost = jaws_staging
queue = nersc_rpc
num_threads = 5
max_retries = 3
[CENTRAL_RPC_CLIENT]
user = jaws_nersc
password = $JAWS_SITE_CORI_RMQ_PW
host = rmq.lbl.gov
port = 5672
vhost = jaws_staging
queue = central_rpc
[GLOBUS]
client_id = 29a47d96-76b4-4f2b-a8bb-a4cb38ea17e3
endpoint_id = 9d6d994a-6d04-11e5-ba46-22000b92c6ec
root_dir = /
default_dir = ~
[SITE]
id = CORI
staging_subdirectory = global/cscratch1/sd/jaws_jtm/staging/staging
results_subdirectory = global/cscratch1/sd/jaws/staging/results
[CROMWELL]
url = http://localhost:50103
EOM

cat << EOM > $configdir/jaws-central.conf
[DB]
dialect = mysql+mysqlconnector
host = jaws-db.lbl.gov
port = 3306
user = jaws_central
password = $JAWS_CENTRAL_CORI_DB_PW
db = jaws_central_staging
[RPC_SERVER]
user = jaws_central
password = $JAWS_CENTRAL_CORI_RMQ_PW
host = rmq.lbl.gov
port = 5672
vhost = jaws_staging
queue = central_rpc
num_threads = 5
max_retries = 3
[GLOBUS]
client_id = 29a47d96-76b4-4f2b-a8bb-a4cb38ea17e3
[HTTP]
auth_port = 3002
rest_port = 5002
[SITE:JGI]
user = jaws_central
password = $JAWS_CENTRAL_CORI_RMQ_PW
host = rmq.lbl.gov
port = 5672
vhost = jaws_staging
queue = lrc_rpc
globus_endpoint = b445e71a-4933-11ea-9714-021304b0cca7
globus_basepath = /global/scratch/jaws
staging_subdir = jtm/staging/staging
max_ram_gb = 1024
[SITE:CORI]
user = jaws_central
password = $JAWS_CENTRAL_CORI_RMQ_PW
host = rmq.lbl.gov
port = 5672
vhost = jaws_staging
queue = nersc_rpc
globus_endpoint = 9d6d994a-6d04-11e5-ba46-22000b92c6ec
globus_basepath = /
staging_subdir = /global/cscratch1/sd/jaws_jtm/staging/staging
max_ram_gb = 2048
EOM

cat << EOM > $configdir/jaws-client.conf
[JAWS]
site_id = NERSC
name = jaws-staging
url = http://cori20.nersc.gov:5003/api/v2
womtool_jar = /global/cfs/projectdirs/jaws/cromwell/womtool.jar
staging_subdir = global/cscratch1/sd/jaws/staging/staging
[GLOBUS]
client_id = 29a47d96-76b4-4f2b-a8bb-a4cb38ea17e3
endpoint_id = 9d6d994a-6d04-11e5-ba46-22000b92c6ec
basedir = /
EOM

cat << EOM > $configdir/jaws-jtm.conf
[SITE]
jtm_host_name = cori
user_name = jtm_staging
instance_name = \${jtm_host_name}.\${user_name}
scratch = /global/cscratch1/sd/jaws_jtm/
debug = 0
[RMQ]
user = jaws_nersc
password = $JAWS_SITE_CORI_RMQ_PW
host = rmq.lbl.gov
port = 5672
vhost = jaws_staging
[SITE_RPC_CLIENT]
user = jaws_nersc
password = $JAWS_SITE_CORI_RMQ_PW
host = rmq.lbl.gov
port = 5672
vhost = jaws_staging
queue = site_rpc
[MYSQL]
host = db.mysql.prod-cattle.stable.spin.nersc.org
port = 60006
user = jtm
password = $JAWS_JTM_CORI_DB_PW
db = jtm_staging
[SLURM]
# number of nodes for the pool
nnodes = 1
# num cores for small task
ncpus = 1
# default wall clock time
jobtime = 00:30:00
mempercpu = 1gb
mempernode = 5gb
partition = lr3
qos = genepool
charge_accnt = fungalp
constraint = haswell
[JTM]
cluster = \${SITE:jtm_host_name}
run_mode = staging
# if STANDALONE == 1, do not sending task status to JAWS Site
standalone = 0
# static worker self cloning time rate. not used.s
clone_time_rate = 0.2
# number of workers per node
num_workers_per_node = 1
log_dir = \${SITE:scratch}/jtm
# config file location of worker
worker_config_file = /global/cfs/projectdirs/jaws/jtm-staging/jaws-jtm.conf
env_activation = source /global/cfs/projectdirs/jaws/jtm-staging/bin/activate
pool_name = small
# Worker setting
# worker's hb expiration (ms)
worker_s_hb_expiration = 60000
# worker->client heartbeat sending interval in worker
worker_hb_send_interval = 2.0
# timeout for waiting the client's heartbeat; 0=no timeout. OBSOLETE
worker_timeout = 0
# zombie worker checking interval
worker_kill_interval = 300.0
# number of procs checking interval
num_procs_check_interval = 60.0
# manager's interval to check if there is task to terminate
task_kill_interval = 5.0
# client->worker heartbeat receiving interval in worker.
worker_hb_recv_interval = 5.0
# tasks status update interval. OBSOLETE
task_stat_update_interval = 0.5
# to ensure updating runs table
runs_info_update_wait = 0.5
# to ensure live worker info is updated in workers' table
worker_info_update_wait = 0.5
# interval to check the result from jtm
result_receive_interval = 0.1
# num threads to recv results from workers
num_result_recv_threads = 1
# client->worker heartbeat sending interval
client_hb_send_interval = 3.0
# live worker checking count limit
worker_hb_check_max_count = 0
# worker->client heartbeat receiving interval
client_hb_recv_interval = 6.0
# max number of trial for checking
file_checking_max_trial = 3
# sleep time between output file checking before retrial
file_check_interval = 3.0
# increase amount of wait time for file checking
file_check_int_inc = 1.5
# jtm_* CLI command timeout in secs.
jtminterface_max_trial = 3600
# worker lifeleft for task timeout
task_kill_timeout_minute = 3
jtm_inner_request_q = _jtm_inner_request_queue.\${SITE:instance_name}
jtm_inner_result_q = _jtm_inner_result_queue.\${SITE:instance_name}
jtm_task_kill_q = _jtm_task_kill.\${SITE:instance_name}
jtm_worker_poison_q = _jtm_worker_poison.\${SITE:instance_name}
jtm_task_result_q = _jtm_task_result_queue.\${SITE:instance_name}
jtm_task_request_q = _jtm_task_request_queue.\${SITE:instance_name}
jtm_status_result_q = _jtm_task_result_queue.\${SITE:instance_name}
jtm_status_request_q = _jtm_task_request_queue.\${SITE:instance_name}
worker_hb_q_postfix = _jtm_worker_hb_queue.\${SITE:instance_name}
client_hb_q_postfix = _jtm_client_hb_queue.\${SITE:instance_name}
jgi_jtm_main_exch = jgi_jtm_main_exch_\${JTM:run_mode}_\${SITE:instance_name}
jtm_client_hb_exch = jgi_jtm_client_hb_exch_\${JTM:run_mode}_\${SITE:instance_name}
jtm_inner_main_exch = jgi_jtm_inner_main_exch_\${JTM:run_mode}_\${SITE:instance_name}
jtm_worker_hb_exch = jgi_jtm_worker_hb_exch_\${JTM:run_mode}_\${SITE:instance_name}
jtm_task_kill_exch = jgi_jtm_task_kill_exch_\${JTM:run_mode}_\${SITE:instance_name}
jtm_worker_poison_exch = jgi_jtm_poison_exch_\${JTM:run_mode}_\${SITE:instance_name}
EOM


cat << EOM > $configdir/cromwell.conf
include required(classpath("application"))
webservice
{
  port = 50102
}
system
{
  abort-jobs-on-terminate = true
  graceful-server-shutdown = true
  workflow-restart = false
  max-concurrent-workflows = 100000
  max-workflow-launch-count = 100000
  new-workflow-poll-rate = 1
  number-of-workflow-log-copy-workers = 20
  number-of-cache-read-workers = 50
  job-rate-control
  {
    jobs = 1
    per = 1 second
  }
}
workflow-options
{
  workflow-log-dir: "/tmp/jaws-staging/logs/cromwell-workflow-logs"
  workflow-log-temporary: false
  workflow-failure-mode: "ContinueWhilePossible"
  default
  {
    workflow-type: WDL
    workflow-type-version: "draft-2"
  }
}
call-caching
{
  enabled = false
  invalidate-bad-cache-result = true
}
# this is required for shifter to find image from its registry.
docker {
    hash-lookup {
        enabled = false
    }
}
backend
{
  default = "JTM"
  providers
  {
    JTM
    {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        exit-code-timeout-seconds = 60
        runtime-attributes = """
        String? docker
        String time = "00:30:00"
        Int cpu = 1
        String mem = "5G"
        String cluster = "cori"
        String poolname = "small"
        String constraint = "haswell"
        String qos = "genepool_special"
        String account = "fungalp"
        Int node = 1
        Int nwpn = 1
        Int shared = 0
        """
        submit = """jtm --config=/tmp/jaws-staging/configs/jaws-jtm.conf \
          submit \
          -cmd '/bin/bash \${script}' \
          -cl \${cluster} \
          -t \${time} \
          -c \${cpu} \
          -m \${mem} \
          -p \${poolname} \
          -C \${constraint} \
          -N \${node} \
          -nwpn \${nwpn} \
          -jid \${job_name} \
          --qos \${qos} \
          -A \${account} \
          --shared \${shared}"""
        kill = "jtm --config=/tmp/jaws-staging/configs/jaws-jtm.conf \
                kill -tid \${job_id}"
        check-alive = "jtm --config=/tmp/jaws-staging/configs/jaws-jtm.conf \
                       isalive -tid \${job_id}"
        job-id-regex = "JTM task ID (\\\d+)"
        submit-docker = """
            LOOKUP=\$(shifterimg lookup \${docker})
            if [[ ! \$LOOKUP ]]; then
                shifterimg pull \${docker}
            fi
            jtm --config=/tmp/jaws-staging/configs/jaws-jtm.conf \
              submit \
              -cmd '/global/cfs/projectdirs/jaws/cromwell/shifter_exec.sh \${docker} \${job_shell} \${script}' \
                    -cl \${cluster} \
                    -t \${time} \
                    -c \${cpu} \
                    -m \${mem} \
                    -p \${poolname} \
                    -C \${constraint} \
              -N \${node} \
              -nwpn \${nwpn} \
              -jid \${job_name} \
              --qos \${qos} \
              -A \${account} \
              --shared \${shared}
        """
        # Root directory where Cromwell writes job results in the container. This value
        # can be used to specify where the execution folder is mounted in the container.
        # it is used for the construction of the docker_cwd string in the submit-docker
        # value above AND in the generation of the "script" file.
        dockerRoot = /global/cscratch1/sd/jaws_jtm/staging/cromwell-executions
      }
    }
  }
}
database
{
  profile = "slick.jdbc.MySQLProfile$"
  db
  {
    driver = "com.mysql.cj.jdbc.Driver"
    url = "jdbc:mysql://db.mysql.prod-cattle.stable.spin.nersc.org:60006/cromwell_staging?rewriteBatchedStatements=true&useSSL=false&autoReconnect=true&useUnicode=true&useJDBCCompliantTimezoneShift=true&useLegacyDatetimeCode=false&serverTimezone=UTC"
    user = "cromwell"
    password = "$JAWS_CROMWELL_CORI_DB_PW"
    connectionTimeout = 5000
  }
  insert-batch-size = 2000
}
EOM
