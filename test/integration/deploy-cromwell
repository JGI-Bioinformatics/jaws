#!/usr/bin/env bash

echo "BEGIN deploy-cromwell"  # noqa


## VERIFY REQUIRED VARS ARE DEFINED
# If any env are undefined, try sourcing the setup script and check again.
# Exits if any required var is undefined.
REQUIRED_VARS="
DEPLOYMENT_NAME
INSTALL_DIR
LOGS_DIR
CONFIG_DIR
CROMWELL_PORT
CROMWELL_JAR_URL
SITE_CROMWELL_JAR
SITE_JTM_GROUP
SITE_CONTAINER_TYPE
SITE_CONTAINERS_TMPDIR
SITE_CONTAINERS_PULLDIR
SITE_JTM_SCRATCH_DIR
SITE_CROMWELL_TMPDIR
CROMWELL_WORKFLOW_LOGS_DIR
CROMWELL_EXECUTIONS_DIR
JTM_CONFIG_FILE
"
RESULT=0
for VAR in $REQUIRED_VARS; do
  if [ -z \${!VAR+xxx} ]; then
    echo "Missing env var, $VAR; sourcing setup script..."
    source ./test/integration/define-env
    RESULT=1
    break
  fi
done

if [[ $RESULT -ne 0 ]]; then
  RESULT=0
  for VAR in $REQUIRED_VARS; do
    if [ -z \${!VAR+xxx} ]; then
      echo "Missing env var: $VAR">&2
      RESULT=1
    fi
  done
fi
[ $RESULT -eq 0 ] || exit 1

## CREATE SITE_REF_DATA_DIR DIR
if [ ! -d "$SITE_REF_DATA_DIR" ]; then
  mkdir -p "$SITE_REF_DATA_DIR"
  fix_perms $SITE_JTM_GROUP "$SITE_REF_DATA_DIR"
fi

## COPY CROMWELL JAR
wget --no-verbose "$CROMWELL_JAR_URL" -O "$SITE_CROMWELL_JAR"
chmod 640 "$SITE_CROMWELL_JAR"
chgrp "$SITE_JTM_GROUP" "$SITE_CROMWELL_JAR"

# When creating the wrapper scripts the arguments to each script are as follows:
# For SHIFTER
# $1 image name
# $2 shell name (eg - bash, csh)
# $3 script name
# $4 site reference data dir
# $5 site fast-scratch dir
# $6 site big-scratch dir
#
# For SINGULARITY
# $1 script name
# $2 docker command
# $3 image name
# $4 shell name (eg - bash, csh)
# $5 script name
# $6 site reference data dir
# $7 site fast-scratch dir
# $8 site big-scratch dir

CONTAINER_WRAPPER="docker run"
## GENRATE CONTAINER HELPER SCRIPT
if [[ "$SITE_CONTAINER_TYPE" == "shifter" ]]; then
    CONTAINER_PULL="
    REPO=\$(echo \${docker} | sed 's/@.*//')
    HASH=\$(echo \${docker} | sed 's/.*@//')
    ID=\$(echo \${docker} | sed 's/.*://')
    IMAGE=\${docker}
    TAG=
    RC=
    PULLED=1
    OUT=

    if [[ \$HASH =~ \"sha256\" ]]; then
         shifter --image=id:\$ID echo testing to see if we already have image > /dev/null 2>&1
         RC=\$?
    else
         shifter --image=\${docker} echo testing to see if we already have image > /dev/null 2>&1
         RC=\$?
    fi

    if [[ \$RC == 0 ]]; then
        echo \"image already pulled: \${docker}.\"
    else
        PULLED=
        # get the version tag from the sha tag so we can pull the image with shifterimg
        if [[ \$HASH =~ \"sha256\" ]]; then
            # Try to figure out the version to pull
            RT=\$(skopeo inspect docker://\${docker} | jq .RepoTags)
            if [[ \$(echo \$RT | grep \"Error\" ) ]]; then
                echo \"Failed to run skopeo for \$docker!\"
                echo \"Error: \$RT.\"
                exit 1
            else
                echo \"successfully pulled image \$IMAGE!\"
            fi
            for ttag in \$(echo \$RT | sed 's/[\",[]//g'); do
                digest=\$(skopeo inspect docker://\$REPO:\$ttag | jq .Digest | sed 's/\"//g')
                if [ \"\$digest\" == \"\$HASH\" ]; then
                    TAG=\$ttag
                    break
                fi
            done

            if [ -z \$TAG ]; then
                echo \"Unable to determine image version.\" 1>&2
                exit 1
            fi

            IMAGE=\$REPO:\$TAG
        else
            IMAGE=\$REPO
        fi
    fi

    # Pull image by tag
    if [[ ! \$PULLED ]]; then
        OUT=\$(shifterimg pull \$IMAGE 2>&1)
        if [[ \$(echo \$OUT | grep \"FAILURE\" ) ]]; then
            echo \"Invalid container name or failed to pull container, \$IMAGE!\"
            echo \"Error: \$OUT.\"
            exit 1
        else
            echo \"successfully pulled image \$IMAGE!\"
        fi
    fi
    "
    CONTAINER_WRAPPER="$INSTALL_DIR/shifter_exec.sh"

    # note that double-quotation marks are required around optional scratch dirs
    CONTAINER_EXEC_COMMAND="$CONTAINER_WRAPPER \$IMAGE \${job_shell} \${script} \"$SITE_REF_DATA_DIR\" \"$SITE_FAST_SCRATCH_DIR\" \"$SITE_BIG_SCRATCH_DIR\""

    cp "./test/integration/shifter_exec.sh" $CONTAINER_WRAPPER
    chgrp "$SITE_JTM_GROUP" "$INSTALL_DIR/shifter_exec.sh"
    chmod 775 "$INSTALL_DIR/shifter_exec.sh"
    
elif [[ "$SITE_CONTAINER_TYPE" == "docker" ]]; then
    CONTAINER_PULL="
    out=\$(docker pull \${docker})
    ret=\$?
    if [[ \$ret != 0 ]]; then
      echo \"Invalid container name or failed to pull container, \${docker}!\" >&2
      echo \"Error: \$out.\"
      exit 1
    else
      echo \"Successfully pulled \${docker}!\"
    fi
    "
    CONTAINER_WRAPPER="$INSTALL_DIR/docker_exec.sh"
    # note that double-quotation marks are required around optional scratch dirs
    CONTAINER_EXEC_COMMAND="$CONTAINER_WRAPPER \${cwd} \${docker_cwd} \${docker} \${job_shell} \${script} \"$SITE_REF_DATA_DIR\" \"$SITE_FAST_SCRATCH_DIR\" \"$SITE_BIG_SCRATCH_DIR\""

    cp "./test/integration/docker_exec.sh" $CONTAINER_WRAPPER
    chgrp "$SITE_JTM_GROUP" "$INSTALL_DIR/docker_exec.sh"
    chmod 775 "$INSTALL_DIR/docker_exec.sh"

elif [[ "$SITE_CONTAINER_TYPE" == "singularity" ]]; then
    CONTAINER_PULL="
    export SINGULARITY_CACHEDIR=$SITE_CONTAINERS_TMPDIR
    export SINGULARITY_PULLFOLDER=$SITE_CONTAINERS_PULLDIR
    export SINGULARITY_TMPDIR=$SITE_CONTAINERS_TMPDIR
    export SINGULARITY_LOCALCACHEDIR=$SITE_CONTAINERS_TMPDIR
    export FLOCK_DIR=/tmp

    IMAGE=\$(echo \${docker} | tr '/:' '_').sif
    echo \"\$IMAGE\"
    if [ -z \$SINGULARITY_CACHEDIR ];
        then CACHE_DIR=\$HOME/.singularity/cache
        else CACHE_DIR=\$SINGULARITY_CACHEDIR
    fi
    # Make sure cache dir exists so lock file can be created by flock
    mkdir -p \$CACHE_DIR
    LOCK_FILE=\$FLOCK_DIR/singularity_pull_flock
    out=\$(flock --exclusive --timeout 900 \$LOCK_FILE \
    singularity pull \$IMAGE docker://\${docker}  2>&1)
    ret=\$?
    if [[ \$ret == 0 ]]; then
        echo \"Successfully pulled \${docker}!\"
    else
        if [[ \$(echo \$out | grep \"exists\" ) ]]; then
            echo \"Image file already exists, \${docker}!\"
        else
            echo \"Failed to pull \${docker}!\" >&2
            echo \"Error: \$out.\"
            exit 1
        fi
    fi
    "
    CONTAINER_WRAPPER="$INSTALL_DIR/singularity_exec.sh"
    # note that double-quotation marks are required around optional scratch dirs
    CONTAINER_EXEC_COMMAND="$CONTAINER_WRAPPER \${cwd} \${docker_cwd} \$SINGULARITY_PULLFOLDER/\$IMAGE \${job_shell} \${script} \"$SITE_REF_DATA_DIR\" \"$SITE_FAST_SCRATCH_DIR\" \"$SITE_BIG_SCRATCH_DIR\""

    cp "./test/integration/singularity_exec.sh" $CONTAINER_WRAPPER
    chgrp "$SITE_JTM_GROUP" "$INSTALL_DIR/singularity_exec.sh"
    chmod 775 "$INSTALL_DIR/singularity_exec.sh"
fi

## GENERATE CONFIG
cat << EOM > $CONFIG_DIR/cromwell.conf
include required(classpath("application"))
webservice
{
  port = $CROMWELL_PORT
  interface = 127.0.0.1
}
system
{
  abort-jobs-on-terminate = true
  graceful-server-shutdown = true
  workflow-restart = false
  max-concurrent-workflows = 100000
  max-workflow-launch-count = 100000
  new-workflow-poll-rate = 1
  number-of-workflow-log-copy-workers = 20
  number-of-cache-read-workers = 50
  job-rate-control
  {
    jobs = 1
    per = 1 second
  }
}
workflow-options
{
  workflow-log-dir: "$CROMWELL_WORKFLOW_LOGS_DIR"
  workflow-log-temporary: false
  workflow-failure-mode: "ContinueWhilePossible"
  default
  {
    workflow-type: WDL
    workflow-type-version: "draft-2"
  }
}
call-caching
{
  enabled = true
  invalidate-bad-cache-result = true
}
# this is required for shifter to find image from its registry.
docker {
    hash-lookup {
        enabled = false
    }
}
backend
{
  default = "HtCondor"
  providers
  {
    LOCAL
    {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        concurrent-job-limit = 0
      }
    }
    HtCondor {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"

      config
      {
        filesystems {
          local {
            localization: ["hard-link", "copy"]
            caching {
              duplication-strategy: ["hard-link", "copy"],
              hashing-strategy: "xxh64"
            }
          }
        }        
        #exit-code-timeout-seconds = 60
        runtime-attributes = """
          String? docker
          String? docker_user
          Int? cpu = 1
          Float? memory_mb = 2048
          Float? disk_kb = 25600.0
          Int? runtime_minutes
          String? nativeSpecs
          String? priority
        """
        submit = """
          export CONDOR_CONFIG="$SITE_HTCONDOR_MASTER_CONFIG"
          PATH="$SITE_HTCONDOR_INSTALL/bin":"$SITE_HTCONDOR_INSTALL/sbin":$PATH
          chmod 755 \\${script}
          #printenv > env.debug          
          cat > \\${cwd}/execution/submitFile <<EOF
PATH="$SITE_HTCONDOR_INSTALL/bin":"$SITE_HTCONDOR_INSTALL/sbin":$PATH
Iwd=\${cwd}/execution
+Owner=UNDEFINED
request_memory=\${memory_mb}
request_disk=\${disk_kb}
request_cpus=\${cpu}
error=\${cwd}/execution/stderr
output=\${cwd}/execution/stdout
log_xml=true
executable=\${script}
log=\${cwd}/execution/execution.log
queue
EOF
          condor_submit \${cwd}/execution/submitFile
        """
        submit-docker = """
          export CONDOR_CONFIG="$SITE_HTCONDOR_MASTER_CONFIG"
          PATH="$SITE_HTCONDOR_INSTALL/bin":"$SITE_HTCONDOR_INSTALL/sbin":$PATH
          $CONTAINER_PULL
          #printenv > env.debug
          cat > \${cwd}/execution/dockerScript <<EOF
#!/bin/bash
PATH="$SITE_HTCONDOR_INSTALL/bin":"$SITE_HTCONDOR_INSTALL/sbin":$PATH
$CONTAINER_EXEC_COMMAND
EOF
          chmod 755 \${cwd}/execution/dockerScript

          # converts working directory into readable and parsable name for htcondor job
          export name=\$(echo \${cwd} | sed 's/cromwell-executions//g' | sed 's/cromwell//g' | sed 's/\/\/\///g' | sed 's/\//_/g' );
          cat > \${cwd}/execution/submitFile <<EOF
JobBatchName = \$name
Iwd=\${cwd}/execution
+Owner=UNDEFINED
request_memory=\${memory_mb}
request_disk=\${disk_kb}
request_cpus=\${cpu}
error=\${cwd}/execution/stderr
output=\${cwd}/execution/stdout
log_xml=true
executable=\${cwd}/execution/dockerScript
log=\${cwd}/execution/execution.log
queue
EOF
          condor_submit \${cwd}/execution/submitFile
       """
        kill = "export CONDOR_CONFIG=$SITE_HTCONDOR_MASTER_CONFIG && $SITE_HTCONDOR_INSTALL/bin/condor_rm \${job_id}"
        check-alive = "export CONDOR_CONFIG=$SITE_HTCONDOR_MASTER_CONFIG && $SITE_HTCONDOR_INSTALL/bin/condor_q \${job_id}"
        job-id-regex = "(?sm).*cluster (\\\d+)..*"
        root = $CROMWELL_EXECUTIONS_DIR
        dockerRoot = $CROMWELL_EXECUTIONS_DIR
      }
    }
  }
}
database
{
  profile = "slick.jdbc.MySQLProfile$"
  db
  {
    driver = "com.mysql.cj.jdbc.Driver"
    url = "jdbc:mysql://$JAWS_DB_HOST:$JAWS_DB_PORT/cromwell_${JAWS_SITE,,}_$DEPLOYMENT_NAME?rewriteBatchedStatements=true&useSSL=false&autoReconnect=true&useUnicode=true&useJDBCCompliantTimezoneShift=true&useLegacyDatetimeCode=false&serverTimezone=UTC"
    user = "jaws"
    password = "$JAWS_DB_PW"
    connectionTimeout = 5000
  }
  insert-batch-size = 2000
}
EOM

chmod 660 "$CONFIG_DIR/cromwell.conf"
chgrp "$SITE_JTM_GROUP" "$CONFIG_DIR/cromwell.conf"


## GENERATE SHIM
CROM_STDOUT="$LOGS_DIR/cromwell.out"
CROM_STDERR="$LOGS_DIR/cromwell.err"
cat <<EOM > "$SHIM_DIR/jaws-cromwell-$DEPLOYMENT_NAME"
#!/usr/bin/env bash

export LC_ALL=en_US.UTF-8
export LANG=en_US.UTF-8
export PYTHONIOENCODING=utf-8

# cromwell needs jtm to be in path
source $INSTALL_DIR/jtm/bin/activate

$SITE_LOAD_JAVA

test -d $SITE_JTM_SCRATCH_DIR || mkdir $SITE_JTM_SCRATCH_DIR
chgrp $SITE_JTM_GROUP $SITE_JTM_SCRATCH_DIR
chmod 2775 $SITE_JTM_SCRATCH_DIR
cd $SITE_JTM_SCRATCH_DIR
exec java -Xmx5g -Dconfig.file="$CONFIG_DIR/cromwell.conf" -Djava.io.tmpdir="$SITE_CROMWELL_TMPDIR" -jar "$INSTALL_DIR/cromwell.jar" server >"$CROM_STDOUT" 2>"$CROM_STDERR"
EOM
chgrp "$SITE_JTM_GROUP" "$SHIM_DIR/jaws-cromwell-$DEPLOYMENT_NAME"
chmod 770 "$SHIM_DIR/jaws-cromwell-$DEPLOYMENT_NAME"

mkdir -p $SITE_JTM_SCRATCH_BASEDIR/condor/run

printf "END deploy-cromwell\n\n"
