#!/usr/bin/env bash

echo "BEGIN deploy-cromwell"

## VERIFY REQUIRED VARS ARE DEFINED
# If any env are undefined, try sourcing the setup script and check again.
# Exits if any required var is undefined.
REQUIRED_VARS="
DEPLOYMENT_NAME
INSTALL_DIR
LOGS_DIR
CONFIG_DIR
CROMWELL_PORT
SITE_CROMWELL_JAR
SITE_JTM_GROUP
SITE_CONTAINER_TYPE
SITE_JTM_SCRATCH_DIR
SITE_JTM_WORKER_INSTALL_DIR
CROMWELL_TMPDIR
CROMWELL_WORKFLOW_LOGS_DIR
CROMWELL_EXECUTIONS_DIR
JTM_CONFIG_FILE
INSTALLED_CROMWELL_JAR
"
RESULT=0
for VAR in $REQUIRED_VARS; do
  if [ -z ${!VAR+xxx} ]; then
    echo "Missing env var, $VAR; sourcing setup script..."
    source ./test/integration/define-env
    RESULT=1
    break
  fi
done
if [[ $RESULT -ne 0 ]]; then
  RESULT=0
  for VAR in $REQUIRED_VARS; do
    if [ -z ${!VAR+xxx} ]; then
      echo "Missing env var: $VAR">&2
      RESULT=1
    fi
  done
fi
[ $RESULT -eq 0 ] || exit 1


## COPY CROMWELL JAR
cp "$SITE_CROMWELL_JAR" "$INSTALLED_CROMWELL_JAR" && \
  chgrp "$SITE_JTM_GROUP" "$INSTALLED_CROMWELL_JAR" && \
  chmod 640 "$INSTALLED_CROMWELL_JAR"

CONTAINER_CHECK=""
if [[ "$SITE_CONTAINER_TYPE" == "shifter" ]]; then
  CONTAINER_CHECK="shifterimg lookup \${docker} || shifterimg pull \${docker}"
fi


# When creating the wrapper scripts the arguments to each script are as follows:
# $1 image name
# $2 shell name (eg - bash, csh)
# $3 script name
# $4 the location of the refdata directory location at the site
# $5 the name of the directory $SITE_REF_DATA_DIR will be mounted on
CONTAINER_WRAPPER="docker run"
## GENRATE CONTAINER HELPER SCRIPT
if [ $SITE_CONTAINER_TYPE = "shifter" ]; then
  CONTAINER_WRAPPER="$SITE_JTM_WORKER_INSTALL_DIR/shifter_exec.sh"
  cat <<EOM > "$CONTAINER_WRAPPER"
#!/usr/bin/env bash
shifter --image=\$1 -V \$2:\$3 \$4 \$5
EOM
  chgrp "$SITE_JTM_GROUP" "$SITE_JTM_WORKER_INSTALL_DIR/shifter_exec.sh"
  chmod 775 "$SITE_JTM_WORKER_INSTALL_DIR/shifter_exec.sh"

elif [ $SITE_CONTAINER_TYPE = "singularity" ]; then
  CONTAINER_WRAPPER="$SITE_JTM_WORKER_INSTALL_DIR/singularity_exec.sh"
  cat <<EOM > "$CONTAINER_WRAPPER"
#!/usr/bin/env bash
export SINGULARITY_CACHEDIR=$SITE_CONTAINERS_DIR
export SINGULARITY_PULLFOLDER=$SITE_CONTAINERS_DIR
export SINGULARITY_TMPDIR=$SITE_CONTAINERS_DIR
export SINGULARITY_LOCALCACHEDIR=$SITE_CONTAINERS_DIR
singularity exec --bind \$2:\$3 docker://\$1 \$4 \$5
exit \$?
EOM
  chgrp "$SITE_JTM_GROUP" "$SITE_JTM_WORKER_INSTALL_DIR/singularity_exec.sh"
  chmod 775 "$SITE_JTM_WORKER_INSTALL_DIR/singularity_exec.sh"
fi

## GENERATE CONFIG
cat << EOM > $CONFIG_DIR/cromwell.conf
include required(classpath("application"))
webservice
{
  port = $CROMWELL_PORT
}
system
{
  abort-jobs-on-terminate = true
  graceful-server-shutdown = true
  workflow-restart = false
  max-concurrent-workflows = 100000
  max-workflow-launch-count = 100000
  new-workflow-poll-rate = 1
  number-of-workflow-log-copy-workers = 20
  number-of-cache-read-workers = 50
  job-rate-control
  {
    jobs = 1
    per = 1 second
  }
}
workflow-options
{
  workflow-log-dir: "$CROMWELL_WORKFLOW_LOGS_DIR"
  workflow-log-temporary: false
  workflow-failure-mode: "ContinueWhilePossible"
  default
  {
    workflow-type: WDL
    workflow-type-version: "draft-2"
  }
}
call-caching
{
  enabled = true
  invalidate-bad-cache-result = true
}
# this is required for shifter to find image from its registry.
docker {
    hash-lookup {
        enabled = false
    }
}
backend
{
  default = "JTM"
  providers
  {
    JTM
    {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        exit-code-timeout-seconds = 60
        runtime-attributes = """
        String? docker
        String time = "00:30:00"
        Int cpu = 32
        String mem = "5G"
        String cluster = "${JAWS_SITE,,}"
        String poolname = "small"
        String constraint = "$SITE_CLUSTER_CONSTRAINT"
        String qos = "$SITE_CLUSTER_QOS"
        String account = "$SITE_CLUSTER_ACCOUNT"
        String partition = "$SITE_CLUSTER_PARTITION"
        Int node = 1
        Int nwpn = 1
        Int shared = 0
        """

        kill = "jtm --config=$JTM_CONFIG_FILE kill -tid \${job_id}"

        check-alive = "jtm --config=$JTM_CONFIG_FILE isalive -tid \${job_id}"

        job-id-regex = "JTM task ID (\\\d+)"

        submit = """
CONSTRAINT=\${constraint}
QOS=\${qos}
PARTITION=\${partition}
[ -z \$QOS ] || QOS="--qos \$QOS"
[ -z \$CONSTRAINT ] || CONSTRAINT="-C \$CONSTRAINT"
[ -z \$PARTITION ] || PARTITION="-P \$PARTITION"
jtm --config=$JTM_CONFIG_FILE \
submit \
-cmd '/bin/bash \${script}' \
-cl \${cluster} \
-t \${time} \
-c \${cpu} \
-m \${mem} \
-p \${poolname} \
-N \${node} \
-nwpn \${nwpn} \
-jid \${job_name} \
-A \${account} \
--shared \${shared} \
\$CONSTRAINT \
\$QOS \
\$PARTITION
"""


        submit-docker = """
CONSTRAINT=\${constraint}
QOS=\${qos}
PARTITION=\${partition}
$CONTAINER_CHECK
[ -z \$QOS ] || QOS="--qos \$QOS"
[ -z \$CONSTRAINT ] || CONSTRAINT="-C \$CONSTRAINT"
[ -z \$PARTITION ] || PARTITION="-P \$PARTITION"
jtm --config=$JTM_CONFIG_FILE \
submit \
-cmd '$CONTAINER_WRAPPER \${docker} $SITE_REF_DATA_DIR /refdata \${job_shell} \${script}' \
-cl \${cluster} \
-t \${time} \
-c \${cpu} \
-m \${mem} \
-p \${poolname} \
-N \${node} \
-nwpn \${nwpn} \
-jid \${job_name} \
-A \${account} \
--shared \${shared} \
\$QOS \
\$CONSTRAINT \
\$PARTITION
"""

        # Root directory where Cromwell writes job results in the container. This value
        # can be used to specify where the execution folder is mounted in the container.
        # it is used for the construction of the docker_cwd string in the submit-docker
        # value above AND in the generation of the "script" file.
        dockerRoot = $CROMWELL_EXECUTIONS_DIR
      }
    }
  }
}
database
{
  profile = "slick.jdbc.MySQLProfile$"
  db
  {
    driver = "com.mysql.cj.jdbc.Driver"
    url = "jdbc:mysql://$SITE_DB_HOST:$SITE_DB_PORT/cromwell_${JAWS_SITE,,}_$DEPLOYMENT_NAME?rewriteBatchedStatements=true&useSSL=false&autoReconnect=true&useUnicode=true&useJDBCCompliantTimezoneShift=true&useLegacyDatetimeCode=false&serverTimezone=UTC"
    user = "jaws"
    password = "$SITE_DB_PW"
    connectionTimeout = 5000
  }
  insert-batch-size = 2000
}
EOM

chmod 660 "$CONFIG_DIR/cromwell.conf"
chgrp "$SITE_JTM_GROUP" "$CONFIG_DIR/cromwell.conf"


## GENERATE SHIM
cat <<EOM > "$SHIM_DIR/jaws-cromwell-$DEPLOYMENT_NAME"
#!/usr/bin/env bash

export LC_ALL=en_US.UTF-8
export LANG=en_US.UTF-8
export PYTHONIOENCODING=utf-8

# cromwell needs jtm to be in path
source $INSTALL_DIR/jtm/bin/activate

$SITE_LOAD_JAVA

test -d $SITE_JTM_SCRATCH_DIR || mkdir $SITE_JTM_SCRATCH_DIR
cd $SITE_JTM_SCRATCH_DIR
exec java -Xmx5g -Dconfig.file="$CONFIG_DIR/cromwell.conf" -Djava.io.tmpdir="$SITE_CROMWELL_TMPDIR" -jar "$INSTALL_DIR/cromwell.jar" server
EOM
chgrp "$SITE_JTM_GROUP" "$SHIM_DIR/jaws-cromwell-$DEPLOYMENT_NAME"
chmod 770 "$SHIM_DIR/jaws-cromwell-$DEPLOYMENT_NAME"

printf "END deploy-cromwell\n\n"
