#!/usr/bin/env bash

echo "BEGIN deploy-cromwell"

## VERIFY REQUIRED VARS ARE DEFINED
# If any env are undefined, try sourcing the setup script and check again.
# Exits if any required var is undefined.
REQUIRED_VARS="
DEPLOYMENT_NAME
INSTALL_DIR
LOGS_DIR
CONFIG_DIR
CROMWELL_PORT
CROMWELL_JAR_URL
SITE_CROMWELL_JAR
SITE_JTM_GROUP
SITE_CONTAINER_TYPE
SITE_CONTAINERS_DIR
SITE_JTM_SCRATCH_DIR
SITE_CROMWELL_TMPDIR
CROMWELL_WORKFLOW_LOGS_DIR
CROMWELL_EXECUTIONS_DIR
JTM_CONFIG_FILE
"
RESULT=0
for VAR in $REQUIRED_VARS; do
  if [ -z ${!VAR+xxx} ]; then
    echo "Missing env var, $VAR; sourcing setup script..."
    source ./test/integration/define-env
    RESULT=1
    break
  fi
done
if [[ $RESULT -ne 0 ]]; then
  RESULT=0
  for VAR in $REQUIRED_VARS; do
    if [ -z ${!VAR+xxx} ]; then
      echo "Missing env var: $VAR">&2
      RESULT=1
    fi
  done
fi
[ $RESULT -eq 0 ] || exit 1


## COPY CROMWELL JAR
wget "$CROMWELL_JAR_URL" -O "$SITE_CROMWELL_JAR"
chmod 640 "$SITE_CROMWELL_JAR"
chgrp "$SITE_JTM_GROUP" "$SITE_CROMWELL_JAR"

# When creating the wrapper scripts the arguments to each script are as follows:
# For SHIFTER
# $1 image name
# $2 shell name (eg - bash, csh)
# $3 script name
# $4 the location of the refdata directory location at the site
# $5 the name of the directory $SITE_REF_DATA_DIR will be mounted on inside container
#
# For SINGULARITY
# $1 the location of the refdata directory location at the site
# $2 the name of the directory $SITE_REF_DATA_DIR will be mounted on inside container
# $3 script name
# $4 docker command
# $5 image name
# $6 shell name (eg - bash, csh)
# $7 script name
#
CONTAINER_WRAPPER="docker run"
## GENRATE CONTAINER HELPER SCRIPT
if [[ "$SITE_CONTAINER_TYPE" == "shifter" ]]; then
  CONTAINER_EXPORT="out=\$(shifterimg lookup \${docker} || shifterimg pull \${docker})
ret=\$?
if [[ \$ret != 0 || \$(echo \$out | grep \"FAILURE\") ]]; then
  echo \"Invalid container name or failed to pull container, \${docker}!\" >&2
  exit \$ret
else
  echo \"Successfully pulled \${docker}!\"
fi
"
  CONTAINER_WRAPPER="$INSTALL_DIR/shifter_exec.sh"
  CONTAINER_EXEC_COMMAND="$CONTAINER_WRAPPER \${docker} $SITE_REF_DATA_DIR /refdata \${job_shell} \${script}"
  cat <<EOM > "$CONTAINER_WRAPPER"
#!/usr/bin/env bash
shifter --image=\$1 -V \$2:\$3 \$4 \$5
EOM
  chgrp "$SITE_JTM_GROUP" "$INSTALL_DIR/shifter_exec.sh"
  chmod 775 "$INSTALL_DIR/shifter_exec.sh"

elif [[ "$SITE_CONTAINER_TYPE" == "singularity" ]]; then
  CONTAINER_EXPORT="export SINGULARITY_CACHEDIR=$SITE_CONTAINERS_DIR
export SINGULARITY_PULLFOLDER=$SITE_CONTAINERS_DIR
export SINGULARITY_TMPDIR=$SITE_CONTAINERS_DIR
export SINGULARITY_LOCALCACHEDIR=$SITE_CONTAINERS_DIR
export FLOCK_DIR=/tmp

IMAGE=\$(echo \${docker} | tr '/:' '_').sif
echo \"\$IMAGE\"
if [ -z \$SINGULARITY_CACHEDIR ];
    then CACHE_DIR=\$HOME/.singularity/cache
    else CACHE_DIR=$\SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p \$CACHE_DIR
LOCK_FILE=\$FLOCK_DIR/singularity_pull_flock
out=\$(flock --exclusive --timeout 900 \$LOCK_FILE \
singularity pull \$IMAGE docker://\${docker}  2>&1)
ret=\$?
if [[ \$ret == 0 ]]; then
    echo \"Successfully pulled \${docker}!\"
else
    if [[ \$(echo \$out | grep \"exists\" ) ]]; then
        echo \"Image file already exists, \${docker}!\"
    else
        echo \"Failed to pull \${docker}!\" >&2
        exit \$ret
    fi
fi
"
  CONTAINER_WRAPPER="$INSTALL_DIR/singularity_exec.sh"
  CONTAINER_EXEC_COMMAND="$CONTAINER_WRAPPER $SITE_REF_DATA_DIR /refdata \${cwd} \${docker_cwd} \$SINGULARITY_PULLFOLDER/\$IMAGE \${job_shell} \${script}"
  cat <<EOM > "$CONTAINER_WRAPPER"
#!/usr/bin/env bash
REMOTE_REF=\$1
LOCAL_REF=\$2
CWD=\$3
DOCKER_CWD=\$4
IMAGE=\$5
JOB_SHELL=\$6
SCRIPT=\$7
singularity exec --bind \$REMOTE_REF:\$LOCAL_REF --bind \$CWD:\$DOCKER_CWD \$IMAGE \$JOB_SHELL \$SCRIPT
exit $?
EOM
  chgrp "$SITE_JTM_GROUP" "$INSTALL_DIR/singularity_exec.sh"
  chmod 775 "$INSTALL_DIR/singularity_exec.sh"
fi

## GENERATE CONFIG
cat << EOM > $CONFIG_DIR/cromwell.conf
include required(classpath("application"))
webservice
{
  port = $CROMWELL_PORT
  interface = 127.0.0.1
}
system
{
  abort-jobs-on-terminate = true
  graceful-server-shutdown = true
  workflow-restart = false
  max-concurrent-workflows = 100000
  max-workflow-launch-count = 100000
  new-workflow-poll-rate = 1
  number-of-workflow-log-copy-workers = 20
  number-of-cache-read-workers = 50
  job-rate-control
  {
    jobs = 1
    per = 1 second
  }
}
workflow-options
{
  workflow-log-dir: "$CROMWELL_WORKFLOW_LOGS_DIR"
  workflow-log-temporary: false
  workflow-failure-mode: "ContinueWhilePossible"
  default
  {
    workflow-type: WDL
    workflow-type-version: "draft-2"
  }
}
call-caching
{
  enabled = true
  invalidate-bad-cache-result = true
}
# this is required for shifter to find image from its registry.
docker {
    hash-lookup {
        enabled = false
    }
}
backend
{
  default = "JTM"
  providers
  {
    LOCAL
    {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        concurrent-job-limit = 0
      }
    }
    JTM
    {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        exit-code-timeout-seconds = 60
        runtime-attributes = """
        String? docker
        String time = "00:30:00"
        Int cpu = 32
        Float? memory_gb = 5
        String cluster = "${JAWS_SITE,,}"
        String poolname = "small"
        String constraint = "$SITE_CLUSTER_CONSTRAINT"
        String qos = "$SITE_CLUSTER_QOS"
        String account = "$SITE_CLUSTER_ACCOUNT"
        String partition = "$SITE_CLUSTER_PARTITION"
        Int node = 1
        Int nwpn = 1
        Int shared = 0
        """

        filesystems {
          local {
            localization: [ "soft-link", "copy" ]
            caching {
              duplication-strategy: [ "soft-link", "copy" ]
              hashing-strategy: "path+modtime"
            }
            http {}
          }
        }

        kill = "jtm --config=$JTM_CONFIG_FILE kill -tid \${job_id}"

        check-alive = "jtm --config=$JTM_CONFIG_FILE isalive -tid \${job_id}"

        job-id-regex = "JTM task ID (\\\d+)"

        submit = """
CONSTRAINT=\${constraint}
QOS=\${qos}
PARTITION=\${partition}
[ -z \$QOS ] || QOS="--qos \$QOS"
[ -z \$CONSTRAINT ] || CONSTRAINT="-C \$CONSTRAINT"
[ -z \$PARTITION ] || PARTITION="-P \$PARTITION"
jtm --config=$JTM_CONFIG_FILE \
submit \
-cmd '/bin/bash \${script}' \
-cl \${cluster} \
-t \${time} \
-c \${cpu} \
-m \${memory_gb} \
-p \${poolname} \
-N \${node} \
-nwpn \${nwpn} \
-jid \${job_name} \
-A \${account} \
--shared \${shared} \
\$CONSTRAINT \
\$QOS \
\$PARTITION
"""


        submit-docker = """
$CONTAINER_EXPORT
CONSTRAINT=\${constraint}
QOS=\${qos}
PARTITION=\${partition}
[ -z \$QOS ] || QOS="--qos \$QOS"
[ -z \$CONSTRAINT ] || CONSTRAINT="-C \$CONSTRAINT"
[ -z \$PARTITION ] || PARTITION="-P \$PARTITION"
jtm --config=$JTM_CONFIG_FILE \
submit \
-cmd "$CONTAINER_EXEC_COMMAND" \
-cl \${cluster} \
-t \${time} \
-c \${cpu} \
-m \${memory_gb} \
-p \${poolname} \
-N \${node} \
-nwpn \${nwpn} \
-jid \${job_name} \
-A \${account} \
--shared \${shared} \
\$QOS \
\$CONSTRAINT \
\$PARTITION
"""

        # Root directory where Cromwell writes job results in the container. This value
        # can be used to specify where the execution folder is mounted in the container.
        # it is used for the construction of the docker_cwd string in the submit-docker
        # value above AND in the generation of the "script" file.
        dockerRoot = $CROMWELL_EXECUTIONS_DIR
      }
    }
  }
}
database
{
  profile = "slick.jdbc.MySQLProfile$"
  db
  {
    driver = "com.mysql.cj.jdbc.Driver"
    url = "jdbc:mysql://$SITE_DB_HOST:$SITE_DB_PORT/cromwell_${JAWS_SITE,,}_$DEPLOYMENT_NAME?rewriteBatchedStatements=true&useSSL=false&autoReconnect=true&useUnicode=true&useJDBCCompliantTimezoneShift=true&useLegacyDatetimeCode=false&serverTimezone=UTC"
    user = "jaws"
    password = "$SITE_DB_PW"
    connectionTimeout = 5000
  }
  insert-batch-size = 2000
}
EOM

chmod 660 "$CONFIG_DIR/cromwell.conf"
chgrp "$SITE_JTM_GROUP" "$CONFIG_DIR/cromwell.conf"


## GENERATE SHIM
CROM_STDOUT="$LOGS_DIR/cromwell.out"
CROM_STDERR="$LOGS_DIR/cromwell.err"
cat <<EOM > "$SHIM_DIR/jaws-cromwell-$DEPLOYMENT_NAME"
#!/usr/bin/env bash

export LC_ALL=en_US.UTF-8
export LANG=en_US.UTF-8
export PYTHONIOENCODING=utf-8

# cromwell needs jtm to be in path
source $INSTALL_DIR/jtm/bin/activate

$SITE_LOAD_JAVA

test -d $SITE_JTM_SCRATCH_DIR || mkdir $SITE_JTM_SCRATCH_DIR
chgrp $SITE_JTM_GROUP $SITE_JTM_SCRATCH_DIR
chmod 2775 $SITE_JTM_SCRATCH_DIR
cd $SITE_JTM_SCRATCH_DIR
exec java -Xmx5g -Dconfig.file="$CONFIG_DIR/cromwell.conf" -Djava.io.tmpdir="$SITE_CROMWELL_TMPDIR" -jar "$INSTALL_DIR/cromwell.jar" server >"$CROM_STDOUT" 2>"$CROM_STDERR"
EOM
chgrp "$SITE_JTM_GROUP" "$SHIM_DIR/jaws-cromwell-$DEPLOYMENT_NAME"
chmod 770 "$SHIM_DIR/jaws-cromwell-$DEPLOYMENT_NAME"

printf "END deploy-cromwell\n\n"
