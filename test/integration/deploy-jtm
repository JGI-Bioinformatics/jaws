#!/usr/bin/env bash

function fix_perms() {
    local GROUP=$1
    local DIR="$2"
    chmod -R a+rX "$DIR"
    chmod -R ug+rwX "$DIR"
    chgrp -R $GROUP "$DIR"
    find "$DIR" -type d -exec chmod g+s '{}' \;
}

echo "BEGIN deploy-jtm"

set -eo pipefail

## ENV VARS
REQUIRED_VARS="
DEPLOYMENT_NAME
JAWS_CENTRAL_RMQ_HOST
JAWS_CENTRAL_RMQ_PORT
JAWS_CENTRAL_RMQ_PW
JAWS_GLOBUS_CLIENT_ID
JAWS_SITE
"
RESULT=0
for VAR in $REQUIRED_VARS; do
  if [ -z ${!VAR+xxx} ]; then
    echo "Missing env var: $VAR">&2
    RESULT=1
  fi
done
[ $RESULT -eq 0 ] || exit 1

echo "DEFINING DEPLOYMENT VARS"
function set_deployment_var {
  VAR_NAME="$1"
  SRC_VAR_NAME="${DEPLOYMENT_NAME}_${VAR_NAME}"
  if [ -z ${!SRC_VAR_NAME+xxx} ]; then
    echo "Missing env var: $SRC_VAR_NAME">&2
    exit 1
  fi
  VALUE="${!SRC_VAR_NAME}"
  echo "... $VAR_NAME"
  export $VAR_NAME
  printf -v "$VAR_NAME" "%s" "$VALUE"
}
DEPLOYMENT_VARS="
LOG_LEVEL
JAWS_SUPERVISOR_PORT
JTM_SUPERVISOR_PORT
JAWS_AUTH_PORT
JAWS_REST_PORT
CROMWELL_PORT"
for DEPLOYMENT_VAR in $DEPLOYMENT_VARS; do
  set_deployment_var $DEPLOYMENT_VAR
done

echo "DEFINING SITE VARS"
function set_site_var {
  VAR_NAME="$1"
  SRC_VAR_NAME="${JAWS_SITE}_${VAR_NAME}"
  if [ -z ${!SRC_VAR_NAME+xxx} ]; then
    echo "Missing env var: $SRC_VAR_NAME">&2
    exit 1
  fi
  SITE_VAR_NAME="SITE_${VAR_NAME}"
  VALUE="${!SRC_VAR_NAME}"
  echo "... $SITE_VAR_NAME"
  export $SITE_VAR_NAME
  printf -v "$SITE_VAR_NAME" "%s" "$VALUE"
}
SITE_VARS="
INSTALL_BASEDIR
GLOBUS_EP
GLOBUS_ROOT_DIR
GLOBUS_DEFAULT_DIR
RMQ_HOST
RMQ_PORT
PYTHON
PYTHON_PIP
LOAD_PYTHON
JAWS_GROUP
JTM_GROUP
CLIENT_GROUP
JAWS_SCRATCH_BASEDIR
JTM_SCRATCH_BASEDIR
JAWS_SW_BASEDIR
JTM_SW_BASEDIR
REF_DATA_DIR
SUPERVISOR_HOST
CONTAINER_TYPE
DB_HOST
DB_PORT
CLUSTER_QOS
CLUSTER_PARTITION
CLUSTER_ACCOUNT
CLUSTER_CONSTRAINT
MAX_RAM_GB
RMQ_PW
DB_PW"
for SITE_VAR in $SITE_VARS; do
  set_site_var $SITE_VAR
done

## DEFINE VARS FROM OTHER VARS
echo "DEFINING PATHS"
export INSTALL_DIR="$SITE_INSTALL_BASEDIR/jaws-$DEPLOYMENT_NAME"
export CONFIG_DIR="$INSTALL_DIR/configs"
export SHIM_DIR="$INSTALL_DIR/shims"
export LOGS_DIR="$INSTALL_DIR/logs"
export SUPERVISOR_DIR="$SITE_INSTALL_BASEDIR/jaws-supervisord-$DEPLOYMENT_NAME"

# Site paths:
export SITE_JTM_SCRATCH_DIR="$SITE_JTM_SCRATCH_BASEDIR/jaws-$DEPLOYMENT_NAME"
export SITE_JTM_WORKER_INSTALL_DIR="$SITE_JTM_SW_BASEDIR/jtm-$DEPLOYMENT_NAME"
export JTM_CONFIG_FILE="$SITE_JTM_WORKER_INSTALL_DIR/jaws-jtm.conf"

## CREATE DIRS AND SET GROUP AND PERMISSIONS
function setup_dir {
  local DIR="$1"
  local GROUP="$2"
  local PERMS="$3"
  [[ -z $DIR ]] && echo "missing DIR" && exit 1
  [[ -z $GROUP ]] && echo "missing GROUP" && exit 1
  echo "... $DIR"
  test -d "$DIR" || mkdir -p "$DIR"
  chgrp "$GROUP" "$DIR"
  chmod "$PERMS" "$DIR"
}
echo "CREATING PATHS"
INSTALL_DIRS="
INSTALL_DIR
CONFIG_DIR
SHIM_DIR
LOGS_DIR
SUPERVISOR_DIR
SITE_JTM_WORKER_INSTALL_DIR
"
for DIR in $INSTALL_DIRS; do
  setup_dir "${!DIR}" "$SITE_JTM_GROUP" 770
done

setup_dir "$SITE_JTM_SCRATCH_DIR" "$SITE_JTM_GROUP" 775
setup_dir "$SITE_JTM_WORKER_INSTALL_DIR" "$SITE_JTM_GROUP" 770



## STOP SERVICE
test -f $SUPERVISOR_DIR/supervisord-jtm.conf && $SUPERVISOR_DIR/bin/supervisorctl -c $SUPERVISOR_DIR/supervisord-jtm.conf stop jaws-$DEPLOYMENT_NAME:*


## GENERATE CONFIG
cat << EOM > $CONFIG_DIR/jaws-jtm.conf
[SITE]
jtm_host_name = $JAWS_SITE
user_name = jtm_$DEPLOYMENT_NAME
instance_name = \${jtm_host_name}.\${user_name}
scratch = $SITE_JTM_SCRATCH_DIR
debug = 1
[RMQ]
user = jaws
password = $SITE_RMQ_PW
host = $SITE_RMQ_HOST
port = $SITE_RMQ_PORT
vhost = jtm_$DEPLOYMENT_NAME
[SITE_RPC_CLIENT]
user = jaws
password = $SITE_RMQ_PW
host = $SITE_RMQ_HOST
port = $SITE_RMQ_PORT
vhost = jaws_$DEPLOYMENT_NAME
queue = SITE_$JAWS_SITE
[MYSQL]
host = $SITE_DB_HOST
port = $SITE_DB_PORT
user = jaws
password = $SITE_DB_PW
db = jtm_${JAWS_SITE,,}_$DEPLOYMENT_NAME
[SLURM]
# number of nodes for the pool
nnodes = 1
# num cores for small task
ncpus = 1
# default wall clock time
jobtime = 00:30:00
mempercpu = 1gb
mempernode = 5gb
partition = $SITE_CLUSTER_PARTITION
qos = $SITE_CLUSTER_QOS
charge_accnt = $SITE_CLUSTER_ACCOUNT
constraint = $SITE_CLUSTER_CONSTRAINT
[JTM]
cluster = \${SITE:jtm_host_name}
run_mode = dev
# if STANDALONE == 1, do not sending task status to JAWS Site
standalone = 0
# static worker self cloning time rate. not used.s
clone_time_rate = 0.2
# number of workers per node
num_workers_per_node = 1
log_dir = \${SITE:scratch}/jtm
# config file location of worker
worker_config_file = $SITE_JTM_WORKER_INSTALL_DIR/jaws-jtm.conf
env_activation = source $SITE_JTM_WORKER_INSTALL_DIR/bin/activate
pool_name = small
# Worker setting
# worker's hb expiration (ms)
worker_s_hb_expiration = 60000
# worker->client heartbeat sending interval in worker
worker_hb_send_interval = 2.0
# timeout for waiting the client's heartbeat; 0=no timeout. OBSOLETE
worker_timeout = 0
# zombie worker checking interval
worker_kill_interval = 300.0
# number of procs checking interval
num_procs_check_interval = 60.0
# manager's interval to check if there is task to terminate
task_kill_interval = 5.0
# client->worker heartbeat receiving interval in worker.
worker_hb_recv_interval = 5.0
# tasks status update interval. OBSOLETE
task_stat_update_interval = 0.5
# to ensure updating runs table
runs_info_update_wait = 0.5
# to ensure live worker info is updated in workers' table
worker_info_update_wait = 0.5
# interval to check the result from jtm
result_receive_interval = 0.1
# num threads to recv results from workers
num_result_recv_threads = 1
# client->worker heartbeat sending interval
client_hb_send_interval = 3.0
# live worker checking count limit
worker_hb_check_max_count = 0
# worker->client heartbeat receiving interval
client_hb_recv_interval = 6.0
# max number of trial for checking
file_checking_max_trial = 3
# sleep time between output file checking before retrial
file_check_interval = 3.0
# increase amount of wait time for file checking
file_check_int_inc = 1.5
# jtm_* CLI command timeout in secs.
jtminterface_max_trial = 3600
# worker lifeleft for task timeout
task_kill_timeout_minute = 3
jtm_inner_request_q = _jtm_inner_request_queue.\${SITE:instance_name}
jtm_inner_result_q = _jtm_inner_result_queue.\${SITE:instance_name}
jtm_task_kill_q = _jtm_task_kill.\${SITE:instance_name}
jtm_worker_poison_q = _jtm_worker_poison.\${SITE:instance_name}
jtm_task_result_q = _jtm_task_result_queue.\${SITE:instance_name}
jtm_task_request_q = _jtm_task_request_queue.\${SITE:instance_name}
jtm_status_result_q = _jtm_status_result_queue.\${SITE:instance_name}
jtm_status_request_q = _jtm_status_request_queue.\${SITE:instance_name}
worker_hb_q_postfix = _jtm_worker_hb_queue.\${SITE:instance_name}
client_hb_q_postfix = _jtm_client_hb_queue.\${SITE:instance_name}
jgi_jtm_main_exch = jgi_jtm_main_exch_\${JTM:run_mode}_\${SITE:instance_name}
jtm_client_hb_exch = jgi_jtm_client_hb_exch_\${JTM:run_mode}_\${SITE:instance_name}
jtm_inner_main_exch = jgi_jtm_inner_main_exch_\${JTM:run_mode}_\${SITE:instance_name}
jtm_worker_hb_exch = jgi_jtm_worker_hb_exch_\${JTM:run_mode}_\${SITE:instance_name}
jtm_task_kill_exch = jgi_jtm_task_kill_exch_\${JTM:run_mode}_\${SITE:instance_name}
jtm_worker_poison_exch = jgi_jtm_poison_exch_\${JTM:run_mode}_\${SITE:instance_name}
EOM
chgrp $SITE_JTM_GROUP $CONFIG_DIR/jaws-jtm.conf
chmod 660 $CONFIG_DIR/jaws-jtm.conf

# Copy the jaws-jtm config to NFS dir since workers will be on cluster node
cp "$CONFIG_DIR/jaws-jtm.conf" "$SITE_JTM_WORKER_INSTALL_DIR/"
chgrp "$SITE_JTM_GROUP" "$SITE_JTM_WORKER_INSTALL_DIR/jaws-jtm.conf"
chmod 660 "$SITE_JTM_WORKER_INSTALL_DIR/jaws-jtm.conf"


## GENERATE SHIMS
cat <<EOM > "$SHIM_DIR/jaws-jtm-$DEPLOYMENT_NAME"
#!/usr/bin/env bash

export LC_ALL=en_US.UTF-8
export LANG=en_US.UTF-8
export PYTHONIOENCODING=utf-8

# DEPLOY TO NFS DIR
# HACK: see .gitlab-ci.yml for the explanation of why that copy does not happen in the CI directly
test -d $SITE_JTM_WORKER_INSTALL_DIR || mkdir $SITE_JTM_WORKER_INSTALL_DIR
test -d $SITE_JTM_SCRATCH_DIR || mkdir $SITE_JTM_SCRATCH_DIR

cp $CONFIG_DIR/jaws-jtm.conf $SITE_JTM_WORKER_INSTALL_DIR/
chgrp $SITE_JTM_GROUP $SITE_JTM_WORKER_INSTALL_DIR/jaws-jtm.conf
chmod 770 $SITE_JTM_WORKER_INSTALL_DIR
chmod 640 $SITE_JTM_WORKER_INSTALL_DIR/jaws-jtm.conf
chmod 775 $SITE_JTM_SCRATCH_DIR

source $INSTALL_DIR/jtm/bin/activate
export JTM_CONFIG_FILE=$CONFIG_DIR/jaws-jtm.conf
exec jtm --config=$CONFIG_DIR/jaws-jtm.conf --debug manager -ld $LOGS_DIR
EOM
chgrp "$SITE_JTM_GROUP" "$SHIM_DIR/jaws-jtm-$DEPLOYMENT_NAME"
chmod +x "$SHIM_DIR"/*

## GENERATE VENVS
rm -rf ./jtm/dist/* ./rpc/dist/*
test -d "$INSTALL_DIR/jtm" && rm -rf "$INSTALL_DIR/jtm"
test -d "$SITE_JTM_WORKER_INSTALL_DIR" && rm -rf "$SITE_JTM_WORKER_INSTALL_DIR"
#[[ -n "$LOAD_SITE_PYTHON" ]] && $LOAD_SITE_PYTHON
$SITE_LOAD_PYTHON
$SITE_PYTHON -m venv "$INSTALL_DIR/jtm"
source "$INSTALL_DIR/jtm/bin/activate"
pip install --upgrade pip
pip install wheel pytest flake8
test -d rpc/dist && rm -rf rpc/dist
test -d jtm/dist && rm -rf jtm/dist
make pkg-rpc
make pkg-jtm
pip install rpc/dist/* && pip install jtm/dist/*
deactivate

$SITE_PYTHON -m venv "$SITE_JTM_WORKER_INSTALL_DIR"
source "$SITE_JTM_WORKER_INSTALL_DIR/bin/activate" && \
pip install --upgrade pip
pip install wheel pytest flake8
test -d rpc/dist && rm -rf rpc/dist
test -d jtm/dist && rm -rf jtm/dist
make pkg-rpc
make pkg-jtm
pip install rpc/dist/* && pip install jtm/dist/*
deactivate


fix_perms $SITE_JTM_GROUP "$INSTALL_DIR/jtm"


printf "END generate-configs\n\n"
