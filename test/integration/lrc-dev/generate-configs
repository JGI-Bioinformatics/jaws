#!/usr/bin/env bash

set -eo pipefail

readonly configdir=${1:-/tmp/}

[[ -z $JAWS_SITE_LBNL_DB_PW ]]     && { printf "Missing JAWS_SITE_LBNL_DB_PW variable.\n"; exit 1; }
[[ -z $JAWS_SITE_LBNL_RMQ_PW ]]    && { printf "Missing JAWS_SITE_LBNL_RMQ_PW variable.\n"; exit 1; }
[[ -z $JAWS_JTM_LBNL_DB_PW ]]      && { printf "Missing JAWS_JTM_LBNL_DB_PW variable.\n"; exit 1; }
[[ -z $JAWS_JTM_LBNL_RMQ_PW ]]     && { printf "Missing JAWS_JTM_LBNL_RMQ_PW variable.\n"; exit 1; }
[[ -z $JAWS_CROMWELL_LBNL_DB_PW ]] && { printf "Missing JAWS_CROMWELL_LBNL_DB_PW variable.\n"; exit 1; }


cat << EOM > $configdir/jaws-site.conf
[DB]
dialect = mysql+mysqlconnector
host = jaws-db.lbl.gov
port = 3306
user = jaws
password = $JAWS_SITE_LBNL_DB_PW
db = jaws_lrc_dev
[LOCAL_RPC_SERVER]
user = jaws
password = $JAWS_JTM_LBNL_RMQ_PW
host = rmq.lbl.gov
port = 5672
vhost = jtm_dev
queue = site_rpc
num_threads = 5
max_retries = 3
[CENTRAL_RPC_SERVER]
user = jaws_lrc
password = $JAWS_SITE_LBNL_RMQ_PW
host = rmq.lbl.gov
port = 5672
vhost = jaws_dev
queue = lrc_rpc
num_threads = 5
max_retries = 3
[CENTRAL_RPC_CLIENT]
user = jaws_lrc
password = $JAWS_SITE_LBNL_RMQ_PW
host = rmq.lbl.gov
port = 5672
vhost = jaws_dev
queue = central_rpc
[GLOBUS]
client_id = 29a47d96-76b4-4f2b-a8bb-a4cb38ea17e3
endpoint_id = b445e71a-4933-11ea-9714-021304b0cca7
root_dir = /global/scratch/jaws
default_dir = /global/scratch/jaws
[SITE]
id = JGI
staging_subdirectory = /global/scratch/jaws/jtm/dev/staging
results_subdirectory = /global/scratch/jaws/jtm/dev/results
[CROMWELL]
url = http://localhost:50101
EOM

cat << EOM > $configdir/jaws-jtm.conf
[SITE]
jtm_host_name = jgi
user_name = jtm_dev
instance_name = \${jtm_host_name}.\${user_name}
scratch = /global/scratch/jaws/jtm/dev
debug = 0
[RMQ]
user = jaws
password = $JAWS_JTM_LBNL_RMQ_PW
host = rmq.lbl.gov
port = 5672
vhost = jtm_dev
[SITE_RPC_CLIENT]
user = jaws
password = $JAWS_JTM_LBNL_RMQ_PW
host = rmq.lbl.gov
port = 5672
vhost = jtm_dev
queue = site_rpc
[MYSQL]
host = jaws-db.lbl.gov
port = 3306
user = jtm
password = $JAWS_JTM_LBNL_DB_PW
db = jtm_lrc_dev
[SLURM]
# number of nodes for the pool
nnodes = 1
# num cores for small task
ncpus = 1
# default wall clock time
jobtime = 00:30:00
mempercpu = 1gb
mempernode = 5gb
partition = lr3
qos = condo_jgicloud
charge_accnt = lr_jgicloud
constraint =
[JTM]
cluster = \${SITE:jtm_host_name}
run_mode = dev
# static worker self cloning time rate. not used.s
clone_time_rate = 0.2
# number of workers per node
num_workers_per_node = 1
log_dir = \${SITE:scratch}/jtm
env_activation = source /global/home/groups-sw/lr_jgicloud/jtm-dev/bin/activate
pool_name = small
# Worker setting
# worker's hb expiration (ms)
worker_s_hb_expiration = 60000
# worker->client heartbeat sending interval in worker
worker_hb_send_interval = 2.0
# timeout for waiting the client's heartbeat; 0=no timeout. OBSOLETE
worker_timeout = 0
# zombie worker checking interval
worker_kill_interval = 300.0
# number of procs checking interval
num_procs_check_interval = 60.0
# manager's interval to check if there is task to terminate
task_kill_interval = 5.0
# client->worker heartbeat receiving interval in worker
worker_hb_recv_interval = 5.0
# tasks status update interval. OBSOLETE
task_stat_update_interval = 0.5
# to ensure updating runs table
runs_info_update_wait = 0.5
# to ensure live worker info is updated in workers' table
worker_info_update_wait = 0.5
# interval to check the result from jtm
result_receive_interval = 0.1
# num threads to recv results from workers
num_result_recv_threads = 1
# client->worker heartbeat sending interval
client_hb_send_interval = 3.0
# live worker checking count limit
worker_hb_check_max_count = 0
# config file location of worker
worker_config_file = /global/home/groups-sw/lr_jgicloud/jtm-dev/jaws-jtm.conf
# worker->client heartbeat receiving interval
client_hb_recv_interval = 6.0
# max number of trial for checking
file_checking_max_trial = 3
# sleep time between output file checking before retrial
file_check_interval = 3.0
# increase amount of wait time for file checking
file_check_int_inc = 1.5
# jtm_* CLI command timeout in secs.
jtminterface_max_trial = 3600
# worker lifeleft for task timeout
task_kill_timeout_minute = 3
jtm_inner_request_q = _jtm_inner_request_queue.\${SITE:instance_name}
jtm_inner_result_q = _jtm_inner_result_queue.\${SITE:instance_name}
jtm_task_kill_q = _jtm_task_kill.\${SITE:instance_name}
jtm_worker_poison_q = _jtm_worker_poison.\${SITE:instance_name}
jtm_task_result_q = _jtm_task_result_queue.\${SITE:instance_name}
jtm_task_request_q = _jtm_task_request_queue.\${SITE:instance_name}
worker_hb_q_postfix = _jtm_worker_hb_queue.\${SITE:instance_name}
client_hb_q_postfix = _jtm_client_hb_queue.\${SITE:instance_name}
jgi_jtm_main_exch = jgi_jtm_main_exch_\${JTM:run_mode}_\${SITE:instance_name}
jtm_client_hb_exch = jgi_jtm_client_hb_exch_\${JTM:run_mode}_\${SITE:instance_name}
jtm_inner_main_exch = jgi_jtm_inner_main_exch_\${JTM:run_mode}_\${SITE:instance_name}
jtm_worker_hb_exch = jgi_jtm_worker_hb_exch_\${JTM:run_mode}_\${SITE:instance_name}
jtm_task_kill_exch = jgi_jtm_task_kill_exch_\${JTM:run_mode}_\${SITE:instance_name}
jtm_worker_poison_exch = jgi_jtm_poison_exch_\${JTM:run_mode}_\${SITE:instance_name}
EOM


cat << EOM > $configdir/cromwell.conf
include required(classpath("application"))

webservice
{
  port = 50101
}

system
{
  abort-jobs-on-terminate = true
  graceful-server-shutdown = true
  workflow-restart = false
  max-concurrent-workflows = 100000
  max-workflow-launch-count = 100000
  new-workflow-poll-rate = 1
  number-of-workflow-log-copy-workers = 20
  number-of-cache-read-workers = 50
  job-rate-control
  {
    jobs = 1
    per = 1 second
  }
}

workflow-options
{
  workflow-log-dir: "/global/scratch/jaws/dev/cromwell-workflow-logs"
  workflow-log-temporary: false
  workflow-failure-mode: "ContinueWhilePossible"
  default
  {
    workflow-type: WDL
    workflow-type-version: "draft-2"
  }
}

call-caching
{
  enabled = false
  invalidate-bad-cache-result = true
}

docker {
    hash-lookup {
        enabled = false
    }
}

backend
{
  default = "JTM"

  providers
  {

    JTM
    {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"

      config {
        exit-code-timeout-seconds = 60
        runtime-attributes = """
        String time = "00:30:00"
        Int cpu = 1
        String mem = "5G"
        String cluster = "jgi"
        String poolname = "small"
        #Int poolsize = 1
        String qos = "condo_jgicloud"
        String account = "lr_jgicloud"
        Int node = 1
        Int nwpn = 1
        Int shared = 0
        String? docker
        """
        submit = """jtm --config=/tmp/jaws-dev/configs/jaws-jtm.conf \
          submit \
          -cmd '/bin/bash \${script}' \
          -cl \${cluster} \
          -t \${time} \
          -c \${cpu} \
          -m \${mem} \
          -p \${poolname} \
          -N \${node} \
          -nwpn \${nwpn} \
          -jid \${job_name} \
          --qos \${qos} \
          -A \${account} \
          --shared \${shared}"""

        kill = "jtm --config=/tmp/jaws-dev/configs/jaws-jtm.conf \
                kill -tid \${job_id}"
        check-alive = "jtm --config=/tmp/jaws-dev/configs/jaws-jtm.conf \
                       isalive -tid \${job_id}"
        job-id-regex = "JTM task ID (\\\d+)"

        submit-docker = """
          ## Build the Docker image into a singularity image, using the head node
          #DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker})
          #IMAGE=/global/scratch/jaws/dev/sif_files/$DOCKER_NAME.sif
          #if [ ! -f $IMAGE ]; then
          #    singularity pull $IMAGE docker://${docker}
          #fi

          ## validate sif was created
          #if [ ! -f $IMAGE ]; then
          #    >&2 echo "Image was not pulled successfully from docker registry: ${docker}"
          #    exit 1
          #fi

          # singularity_exec.sh is required to circumvent an issue with special characters in the singularity
          # command AND it sets some environmental variables to let singularity know where
          # pull temp image layers so we don't get the 'out of memory' error.
          jtm --config=/tmp/jaws-dev/configs/jaws-jtm.conf \
              submit \
              -cmd '/global/home/groups-sw/lr_jgicloud/cromwell/singularity_exec.sh \${docker} \${job_shell} \${script}' \
                    -cl \${cluster} \
                    -t \${time} \
                    -c \${cpu} \
                    -m \${mem} \
                    -p \${poolname} \
                    -N \${node} \
                    -nwpn \${nwpn} \
                    -jid \${job_name} \
                    --qos \${qos} \
                    -A \${account} \
                    --shared \${shared}
        """

        root = "/global/scratch/jaws/dev/cromwell-executions"
        dockerRoot = "/cromwell-executions"
      } # config
    } # JTM
  } # providers
} # backend

database
{
  profile = "slick.jdbc.MySQLProfile$"
  db
  {
    driver = "com.mysql.cj.jdbc.Driver"
    url = "jdbc:mysql://jaws-db.lbl.gov:3306/cromwell_dev?rewriteBatchedStatements=true&autoReconnect=true&useUnicode=true&useJDBCCompliantTimezoneShift=true&useLegacyDatetimeCode=false&serverTimezone=UTC"
    user = "cromwell"
    password = "$JAWS_CROMWELL_LBNL_DB_PW"
    connectionTimeout = 10000
  }
  insert-batch-size = 2000
}

EOM
