include required(classpath("application"))
webservice
{
  port = {{site.local.cromwell.port}}
}
system
{
  abort-jobs-on-terminate = true
  graceful-server-shutdown = true
  workflow-restart = false
  max-concurrent-workflows = 100000
  max-workflow-launch-count = 100000
  new-workflow-poll-rate = 1
  number-of-workflow-log-copy-workers = 20
  number-of-cache-read-workers = 50
  job-rate-control
  {
    jobs = 1
    per = 1 second
  }
}
workflow-options
{
  workflow-log-dir: {{site.local.cromwell.workflow_log_dir}}
  workflow-log-temporary: false
  workflow-failure-mode: "ContinueWhilePossible"
  default
  {
    workflow-type: WDL
    workflow-type-version: "draft-2"
  }
}
call-caching
{
  enabled = false
  invalidate-bad-cache-result = true
}
# this is required for shifter to find image from its registry.
docker {
    hash-lookup {
        enabled = false
    }
}
backend
{
  default = "{{site.local.cromwell.backend}}"
  providers
  {
    JTM_CORI
    {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        exit-code-timeout-seconds = 60
        runtime-attributes = """
        String? docker
        String time = "00:30:00"
        Int cpu = 1
        String mem = "5G"
        String cluster = "cori"
        String poolname = "small"
        String constraint = "haswell"
        String qos = "genepool_special"
        String account = "fungalp"
        Int node = 1
        Int nwpn = 1
        Int shared = 0
        """
        submit = """jtm --config={{site.local.jtm.config}} \
          submit \
          -cmd '/bin/bash \${script}' \
          -cl \${cluster} \
          -t \${time} \
          -c \${cpu} \
          -m \${mem} \
          -p \${poolname} \
          -C \${constraint} \
          -N \${node} \
          -nwpn \${nwpn} \
          -jid \${job_name} \
          --qos \${qos} \
          -A \${account} \
          --shared \${shared}"""
        kill = "jtm --config={{site.local.jtm.config}}  \
                kill -tid \${job_id}"
        check-alive = "jtm --config={{site.local.jtm.config}}  \
                       isalive -tid \${job_id}"
        job-id-regex = "JTM task ID (\\\d+)"
        submit-docker = """
            LOOKUP=\$(shifterimg lookup \${docker})
            if [[ ! \$LOOKUP ]]; then
                shifterimg pull \${docker}
            fi
            jtm --config={{site.local.jtm.config}}  \
              submit \
              -cmd '/global/cfs/projectdirs/jaws/cromwell/shifter_exec.sh \${docker} \${job_shell} \${script}' \
                    -cl \${cluster} \
                    -t \${time} \
                    -c \${cpu} \
                    -m \${mem} \
                    -p \${poolname} \
                    -C \${constraint} \
              -N \${node} \
              -nwpn \${nwpn} \
              -jid \${job_name} \
              --qos \${qos} \
              -A \${account} \
              --shared \${shared}
        """
        # Root directory where Cromwell writes job results in the container. This value
        # can be used to specify where the execution folder is mounted in the container.
        # it is used for the construction of the docker_cwd string in the submit-docker
        # value above AND in the generation of the "script" file.
        dockerRoot = {{site.local.cromwell.docker_root}}
      } # config
    } # JTM_CORI

    JTM_JGI
    {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"

      config {
        exit-code-timeout-seconds = 60
        runtime-attributes = """
        String time = "00:30:00"
        Int cpu = 1
        String mem = "5G"
        String cluster = "jgi"
        String poolname = "small"
        #Int poolsize = 1
        String qos = "condo_jgicloud"
        String account = "lr_jgicloud"
        Int node = 1
        Int nwpn = 1
        Int shared = 0
        String? docker
        """
        submit = """jtm --config={{site.local.jtm.config}} \
          submit \
          -cmd '/bin/bash \${script}' \
          -cl \${cluster} \
          -t \${time} \
          -c \${cpu} \
          -m \${mem} \
          -p \${poolname} \
          -N \${node} \
          -nwpn \${nwpn} \
          -jid \${job_name} \
          --qos \${qos} \
          -A \${account} \
          --shared \${shared}"""

        kill = "jtm --config={{site.local.jtm.config}} \
                kill -tid \${job_id}"
        check-alive = "jtm --config={{site.local.jtm.config}} \
                       isalive -tid \${job_id}"
        job-id-regex = "JTM task ID (\\\d+)"

        submit-docker = """
          ## Build the Docker image into a singularity image, using the head node
          #DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker})
          #IMAGE=/global/scratch/jaws/dev/sif_files/$DOCKER_NAME.sif
          #if [ ! -f $IMAGE ]; then
          #    singularity pull $IMAGE docker://${docker}
          #fi

          ## validate sif was created
          #if [ ! -f $IMAGE ]; then
          #    >&2 echo "Image was not pulled successfully from docker registry: ${docker}"
          #    exit 1
          #fi

          # singularity_exec.sh is required to circumvent an issue with special characters in the singularity
          # command AND it sets some environmental variables to let singularity know where
          # pull temp image layers so we don't get the 'out of memory' error.
          jtm --config={{site.local.jtm.config}} \
              submit \
              -cmd '/global/home/groups-sw/lr_jgicloud/cromwell/singularity_exec.sh \${docker} \${job_shell} \${script}' \
                    -cl \${cluster} \
                    -t \${time} \
                    -c \${cpu} \
                    -m \${mem} \
                    -p \${poolname} \
                    -N \${node} \
                    -nwpn \${nwpn} \
                    -jid \${job_name} \
                    --qos \${qos} \
                    -A \${account} \
                    --shared \${shared}
        """

        root = {{site.local.cromwell.root}}
        dockerRoot = {{site.local.cromwell.docker_root}}
      } # config
    } # JTM_JGI
  }
}
database
{
  profile = "slick.jdbc.MySQLProfile$"
  db
  {
    driver = "com.mysql.cj.jdbc.Driver"
    url = "{{site.local.cromwell.mysql.url}}"
    user = "{{site.local.cromwell.mysql.user}}"
    password = "{{site.local.cromwell.mysql.password}}"
    connectionTimeout = 5000
  }
  insert-batch-size = 2000
}
