"""
JAWS Analysis/Run management functions; these interact via REST with the JAWS Central server.
"""

import os
import json
import requests
import click
import logging
import uuid
import shutil
from typing import Dict

from jaws_client import config, user, workflow


class AnalysisError(Exception):
    def __init__(self, message):
        super().__init__(message)


def request(rest_op, url, data={}, files={}):
    """Perform specified REST operation.  A JSON response is expected."""
    current_user = user.User()
    header = current_user.header()
    response = None
    try:
        if rest_op == "GET":
            response = requests.get(url, headers=header)
        elif rest_op == "PUT":
            response = requests.put(url, headers=header)
        elif rest_op == "POST":
            response = requests.post(url, data=data, files=files, headers=header)
        else:
            raise ValueError(f"Unsupported REST request type: {rest_op}")
    except requests.exceptions.Timeout as err:
        raise SystemExit("Unable to communicate with JAWS server (timeout)", err)
    except requests.exceptions.TooManyRedirects as err:
        raise SystemExit(
            "Unable to communicate with JAWS server (too many redirects; bad url?)", err
        )
    except requests.exceptions.HTTPError as err:
        raise SystemExit("Unable to communicate with JAWS server (http error)", err)
    except requests.exceptions.RequestException as err:
        raise SystemExit("Unable to communicate with JAWS server", err)
    if response.status_code < 200 or response.status_code > 299:
        try:
            result = response.json()
        except Exception:
            raise SystemExit(response.text)
        if "error" in result:
            raise SystemExit(result["error"])
        else:
            raise SystemExit(result)
    return response.json()


def print_json(j):
    print(json.dumps(j, indent=4, sort_keys=True))


@click.group(context_settings={"help_option_names": ["-h", "--help"]})
def runs():
    """JAWS Workflow Commands"""
    pass


@runs.command()
@click.option(
    "--site", default=None, help="limit results to this compute-site; default=all"
)
def queue(site: str) -> None:
    """List of user's current runs"""
    data = {"active_only": True}
    if site:
        data["site_id"] = site.upper()
    url = f'{config.conf.get("JAWS", "url")}/search'
    result = request("POST", url, data)
    print_json(result)


@runs.command()
@click.option("--days", default=1, help="history going back this many days; default=1")
@click.option(
    "--site", default=None, help="limit results to this compute-site; default=all"
)
def history(days: int, site: str) -> None:
    """Print a list of the user's past runs."""
    if days < 1:
        raise SystemExit("User error: --days must be a positive integer")
    data = {"delta_days": days}
    if site:
        data["site_id"] = site.upper()
    url = f'{config.conf.get("JAWS", "url")}/search'
    result = request("POST", url, data)
    print_json(result)


def _run_status(run_id: int) -> Dict[str, str]:
    """Return the status of a run in JSON format."""

    url = f'{config.conf.get("JAWS", "url")}/run/{run_id}'
    return request("GET", url)


@runs.command()
@click.argument("run_id")
def status(run_id: int) -> None:
    """Print the current status of a run."""

    result = _run_status(run_id)
    print_json(result)


@runs.command()
@click.argument("run_id")
@click.option("--fmt", default="text", help="the desired output format: [text|json]")
def task_status(run_id: int, fmt: str) -> None:
    """Show the current status of each Task."""

    url = f'{config.conf.get("JAWS", "url")}/run/{run_id}/task_status'
    result = request("GET", url)
    if fmt == "json":
        print_json(result)
    else:
        print(
            "#CROMWELL_RUN_ID\tTASK_NAME\tATTEMPT\tCROMWELL_JOB_ID\tSTATUS_FROM\tSTATUS_TO\tTIMESTAMP\tREASON"
        )
        for row in result:
            row[2] = str(row[2])
            row[3] = str(row[3])
            print("\t".join(row))


@runs.command()
@click.argument("run_id")
def metadata(run_id: int) -> None:
    """ Print detailed metadata for a run, generated by cromwell. """

    url = f'{config.conf.get("JAWS", "url")}/run/{run_id}/metadata'
    result = request("GET", url)
    print_json(result)


@runs.command()
@click.argument("run_id")
@click.option("--fmt", default="text", help="the desired output format: [text|json]")
def log(run_id: int, fmt: str) -> None:
    """View the log of Run state transitions for the workflow as a whole."""

    url = f'{config.conf.get("JAWS", "url")}/run/{run_id}/run_log'
    result = request("GET", url)
    if fmt == "json":
        print_json(result)
    else:
        print("#STATUS_FROM\tSTATUS_TO\tTIMESTAMP\tREASON")
        for log_entry in result:
            print("\t".join(log_entry))


@runs.command()
@click.argument("run_id")
@click.option("--fmt", default="text", help="the desired output format: [text|json]")
def task_log(run_id: int, fmt: str) -> None:
    """Get log of each Task's state transitions."""

    url = f'{config.conf.get("JAWS", "url")}/run/{run_id}/task_log'
    result = request("GET", url)
    if fmt == "json":
        print_json(result)
    else:
        print(
            "#CROMWELL_RUN_ID\tTASK_NAME\tATTEMPT\tCROMWELL_JOB_ID\tSTATUS_FROM\tSTATUS_TO\tTIMESTAMP\tREASON"
        )
        for row in result:
            row[2] = str(row[2])
            row[3] = str(row[3])
            print("\t".join(row))


@runs.command()
@click.argument("run_id")
@click.option("--fmt", default="text", help="the desired output format: [text|json]")
def errors(run_id: int, fmt: str) -> None:
    """View error messages and stderr for failed Tasks."""

    url = f'{config.conf.get("JAWS", "url")}/run/{run_id}/errors'
    result = request("GET", url)
    if fmt == "json":
        print_json(result)
    else:
        for task_name in result:
            print(f"{task_name}:")
            print(result[task_name])
            print("\n")


@runs.command()
@click.argument("run_id")
def cancel(run_id):
    """Cancel a run; prints whether aborting was successful or not."""

    url = f'{config.conf.get("JAWS", "url")}/run/{run_id}/cancel'
    result = request("PUT", url)
    print_json(result)


@runs.command()
def cancel_all():
    """Cancel all active runs."""
    url = f'{config.conf.get("JAWS", "url")}/run/cancel-all'
    result = request("PUT", url)
    print_json(result)


def _list_sites() -> None:
    """List available Sites."""

    url = f'{config.conf.get("JAWS", "url")}/site'
    result = request("GET", url)
    print("Available Sites:")
    for a_site_id in result:
        print(f"  - {a_site_id}")


@runs.command()
def list_sites() -> None:
    """List available compute Sites"""
    _list_sites()


@runs.command()
@click.argument("wdl_file", nargs=1)
@click.argument("json_file", nargs=1)
@click.argument("site", nargs=1)
@click.option("--tag", help="identifier for the run")
@click.option("--no-cache", is_flag=True, help="Disable call-caching for this run")
def submit(wdl_file: str, json_file: str, site: str, tag: str, no_cache: bool):
    """Submit a run for execution at a JAWS-Site.
       Available sites can be found by running 'jaws run list-sites'.
    """
    logger = logging.getLogger(__package__)

    # the users' jaws id may not match the linux uid where the client is installed
    url = f'{config.conf.get("JAWS", "url")}/user'
    result = request("GET", url)
    uid = result["uid"]

    staging_subdir = config.Configuration().get("JAWS", "staging_dir")
    staging_user_subdir = os.path.join(staging_subdir, uid)
    globus_host_path = config.Configuration().get("GLOBUS", "host_path")
    output_directory = config.conf.get("JAWS", "data_repo_basedir")
    input_site_id = config.conf.get("JAWS", "site_id")
    local_staging_endpoint = workflow.join_path(globus_host_path, staging_user_subdir)

    # GET SITE INFO
    compute_site_id = site.upper()
    url = f'{config.conf.get("JAWS", "url")}/site/{compute_site_id}'
    result = request("GET", url)
    compute_basedir = result["globus_host_path"]
    compute_uploads_subdir = result["uploads_dir"]
    compute_max_ram_gb = int(result["max_ram_gb"])

    # VALIDATE WORKFLOW WDLs
    submission_id = str(uuid.uuid4())
    try:
        wdl = workflow.WdlFile(wdl_file, submission_id)
    except workflow.WdlError as error:
        raise SystemExit(f"There is a problem with your workflow:\n{error}")
    try:
        wdl.validate()
    except workflow.WdlError as error:
        raise SystemExit(error)
    max_ram_gb = wdl.max_ram_gb
    if max_ram_gb > compute_max_ram_gb:
        raise SystemExit(
            f"The workflow requires {max_ram_gb}GB but {compute_site_id} has only {compute_max_ram_gb}GB available"
        )

    # any and all subworkflow WDL files must be supplied to Cromwell in a single ZIP archive
    try:
        staged_wdl, zip_file = workflow.compress_wdls(wdl, local_staging_endpoint)
    except IOError as error:
        raise SystemExit(f"Unable to copy WDLs to inputs dir: {error}")

    # VALIDATE INPUTS JSON
    try:
        inputs_json = workflow.WorkflowInputs(json_file, submission_id)
    except json.JSONDecodeError as error:
        raise SystemExit(f"Your file, {json_file}, is not a valid JSON file: {error}")

    staged_json = workflow.join_path(local_staging_endpoint, f"{submission_id}.json")
    site_subdir = workflow.join_path(local_staging_endpoint, input_site_id)
    jaws_site_staging_dir = workflow.join_path(compute_basedir, compute_uploads_subdir)
    jaws_site_staging_site_subdir = workflow.join_path(
        jaws_site_staging_dir, input_site_id
    )

    # copy infiles in inputs json to site's inputs dir so they may be read by jaws user and
    # transferred to the compute site via globus
    moved_files = inputs_json.move_input_files(site_subdir)

    # the paths in the inputs json file are changed to their paths at the compute site
    modified_json = inputs_json.prepend_paths_to_json(jaws_site_staging_site_subdir)
    modified_json.write_to(staged_json)

    # the original inputs json file is kept with the run submission for record-keeping only
    orig_json = workflow.join_path(local_staging_endpoint, f"{submission_id}.orig.json")
    try:
        shutil.copy(json_file, orig_json)
    except IOError as error:
        raise SystemExit(f"Error copying JSON to {orig_json}: {error}")
    try:
        os.chmod(orig_json, 0o0664)
    except PermissionError as error:
        raise SystemExit(f"Unable to chmod {orig_json}: {error}")

    # turning off call-caching requires a Cromwell options json file be created
    options_json_file = None
    if no_cache is True:
        options_json_file = workflow.join_path(
            local_staging_endpoint, f"{submission_id}.options.json"
        )
        with open(options_json_file, "w") as fh:
            fh.write('{"read_from_cache": false, "write_to_cache": false}')

    # write the file transfer manifest; jaws-central shall submit the transfer to globus
    manifest_file = workflow.Manifest(local_staging_endpoint, compute_uploads_subdir)
    manifest_file.add(staged_wdl, zip_file, staged_json, orig_json, *moved_files)
    if options_json_file:
        manifest_file.add(options_json_file)
    staged_manifest = workflow.join_path(staging_user_subdir, f"{submission_id}.tsv")
    manifest_file.write_to(staged_manifest)

    # SUBMIT RUN TO CENTRAL
    local_endpoint_id = config.conf.get("GLOBUS", "endpoint_id")
    data = {
        "site_id": compute_site_id,
        "submission_id": submission_id,
        "input_site_id": input_site_id,
        "input_endpoint": local_endpoint_id,
        "output_endpoint": local_endpoint_id,  # return to original submission site
        "output_dir": output_directory,  # jaws-writable dir to initially receive results
        "wdl_file": wdl_file,
        "json_file": json_file,
        "tag": tag,
    }
    files = {"manifest": open(staged_manifest, "r")}
    url = f'{config.conf.get("JAWS", "url")}/run'
    logger.debug(f"Submitting run: {data}")
    result = request("POST", url, data, files)
    if "run_id" not in result:
        raise SystemExit(f"Run submission failed: {result}")
    run_id = result["run_id"]
    logger.info(f"Submitted run {run_id}: {data}")
    print(f"Submitted run {run_id}")


@runs.command()
@click.argument("wdl_file", nargs=1)
def inputs(wdl_file: str) -> None:
    """Generate inputs template (JSON) from workflow (WDL) file."""

    if not os.path.isfile(wdl_file):
        raise IOError(f"File not found: {wdl_file}")
    stdout, stderr = workflow.womtool("inputs", wdl_file)
    if stderr:
        raise SystemExit(stderr)
    print(stdout.strip())


@runs.command()
@click.argument("wdl_file", nargs=1)
def validate(wdl_file: str) -> None:
    """Validate a WDL using Cromwell's WOMTool."""

    if not os.path.isfile(wdl_file):
        raise IOError(f"File not found: {wdl_file}")
    stdout, stderr = workflow.womtool("inputs", wdl_file)
    if stderr:
        raise SystemExit(stderr)
    else:
        print("Workflow is OK")


@runs.command()
@click.argument("run_id")
@click.argument("dest")
def get(run_id: int, dest: str) -> None:
    """Copy the output of a run to the specified folder."""

    logger = logging.getLogger(__package__)
    result = _run_status(run_id)
    status = result["status"]
    src = result["output_dir"]

    if status != "download complete":
        raise SystemExit(
            f"Run {run_id} output is not yet available; status is {status}"
        )

    if src is None:
        logger.error(f"Run {run_id} doesn't have an output_dir defined")
        raise SystemExit(f"Run {run_id} doesn't have an output_dir defined")

    try:
        result = workflow.rsync(
            src,
            dest,
            ["-rLtq", "--chmod=Du=rwx,Dg=rwx,Do=,Fu=rw,Fg=rw,Fo=", "--exclude=inputs"],
        )
    except IOError as error:
        logger.error(f"Rsync output failed for run {run_id}: {error}")
        raise SystemExit(f"Error getting output for run {run_id}: {error}")
    if result.returncode != 0:
        err_msg = f"Failed to rsync {src}->{dest}: {result.stdout}; {result.stderr}"
        raise SystemExit(err_msg)
