# include the application.conf at the top
include required(classpath("application"))

system
{
  abort-jobs-on-terminate = true
  graceful-server-shutdown = true
  workflow-restart = false
  max-concurrent-workflows = 100000
  max-workflow-launch-count = 100000
  new-workflow-poll-rate = 1
  number-of-workflow-log-copy-workers = 20
  number-of-cache-read-workers = 50
  job-rate-control
  {
    jobs = 1
    per = 1 second
  }
}
call-caching
{
  enabled = true
  invalidate-bad-cache-result = true
}
# this is required for shifter to find image from its registry.
docker {
    hash-lookup {
        enabled = false
    }
}

backend {
    default: local
    providers: {
        local {
            actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"

            config {
                run-in-background = true
                exit-code-timeout-seconds = 60
  
                runtime-attributes = """
                String? docker
                """

                submit = "/bin/bash ${script}"

                submit-docker = """
                export SINGULARITY_CACHEDIR=/tmp/singularity_files
                export SINGULARITY_PULLFOLDER=/tmp/singularity_files
                export SINGULARITY_TMPDIR=/tmp/singularity_files
                export SINGULARITY_LOCALCACHEDIR=/tmp/singularity_files
                export FLOCK_DIR=/tmp
                
                IMAGE=$(echo ${docker} | tr '/:' '_').sif
                echo "$IMAGE"
                if [ -z $SINGULARITY_CACHEDIR ];
                    then CACHE_DIR=$HOME/.singularity/cache
                    else CACHE_DIR=$SINGULARITY_CACHEDIR
                fi
                # Make sure cache dir exists so lock file can be created by flock
                mkdir -p $CACHE_DIR
                LOCK_FILE=$FLOCK_DIR/singularity_pull_flock
                out=$(flock --exclusive --timeout 900 $LOCK_FILE singularity pull $IMAGE docker://${docker}  2>&1)
                ret=$?
                if [[ $ret == 0 ]]; then
                    echo "Successfully pulled ${docker}!"
                else
                    if [[ $(echo $out | grep "exists" ) ]]; then
                        echo "Image file already exists, ${docker}!"
                    else
                        echo "Failed to pull ${docker}!" >&2
                        exit $ret
                    fi
                fi

                singularity exec --containall \
                --bind ${cwd}:${docker_cwd} \
                --bind /global/scratch/jaws/refdata:/refdata \
                docker://${docker} ${job_shell} ${script}
                """

                kill = "scancel ${job_id}"
                check-alive = "squeue -j ${job_id}"
                job-id-regex = "Submitted batch job (\\d+).*"

                # Root directory where Cromwell writes job results in the container. This value
                # can be used to specify where the execution folder is mounted in the container.
                # it is used for the construction of the docker_cwd string in the submit-docker
                # value above AND in the generation of the "script" file.
                dockerRoot = /cromwell-executions
            }
        } # local

        slurm {
            actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"

            config {
                run-in-background = true
                exit-code-timeout-seconds = 60
  
                runtime-attributes = """
                String? docker
                String time = "00:10:00"
                String mem = "5g"
                Int cpu = 1
                """

                submit = """
                sbatch -A lr_jgicloud -J slurm_exp -p lr3 -q condo_jgicloud -t ${time} -c ${cpu} --mem=${mem} --wrap "/bin/bash ${script}"
                """

                submit-docker = """
                export SINGULARITY_CACHEDIR=/tmp/singularity_files
                export SINGULARITY_PULLFOLDER=/tmp/singularity_files
                export SINGULARITY_TMPDIR=/tmp/singularity_files
                export SINGULARITY_LOCALCACHEDIR=/tmp/singularity_files
                export FLOCK_DIR=/tmp
                
                IMAGE=$(echo ${docker} | tr '/:' '_').sif
                echo "$IMAGE"
                if [ -z $SINGULARITY_CACHEDIR ];
                    then CACHE_DIR=$HOME/.singularity/cache
                    else CACHE_DIR=$SINGULARITY_CACHEDIR
                fi
                # Make sure cache dir exists so lock file can be created by flock
                mkdir -p $CACHE_DIR
                LOCK_FILE=$FLOCK_DIR/singularity_pull_flock
                out=$(flock --exclusive --timeout 900 $LOCK_FILE singularity pull $IMAGE docker://${docker}  2>&1)
                ret=$?
                if [[ $ret == 0 ]]; then
                    echo "Successfully pulled ${docker}!"
                else
                    if [[ $(echo $out | grep "exists" ) ]]; then
                        echo "Image file already exists, ${docker}!"
                    else
                        echo "Failed to pull ${docker}!" >&2
                        exit $ret
                    fi
                fi

                sbatch -A lr_jgicloud -J slurm_exp -p lr3 -q condo_jgicloud -t ${time} -c ${cpu} --mem=${mem} --wrap " \
                singularity exec --containall \
                --bind ${cwd}:${docker_cwd} \
                --bind /global/scratch/jaws/refdata:/refdata \
                docker://${docker} ${job_shell} ${script}
                "
                """

                kill = "scancel ${job_id}"
                check-alive = "squeue -j ${job_id}"
                job-id-regex = "Submitted batch job (\\d+).*"

                # Root directory where Cromwell writes job results in the container. This value
                # can be used to specify where the execution folder is mounted in the container.
                # it is used for the construction of the docker_cwd string in the submit-docker
                # value above AND in the generation of the "script" file.
                dockerRoot = /cromwell-executions
            }
        } # slurm
    }
}

database {
  profile = "slick.jdbc.HsqldbProfile$"
  db {
    driver = "org.hsqldb.jdbcDriver"
    url = """ 
    jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;
    shutdown=false;
    hsqldb.default_table_type=cached;hsqldb.tx=mvcc;
    hsqldb.result_max_memory_rows=10000;
    hsqldb.large_data=true;
    hsqldb.applog=1;
    hsqldb.lob_compressed=true;
    hsqldb.script_format=3
    """ 
    connectionTimeout = 120000
    numThreads = 1 
   }   
}
