# include the application.conf at the top
include required(classpath("application"))

docker {
    hash-lookup {
      enabled = false
    }
}

include required(classpath("application"))

backend {
    default: singularity
    providers: {
        singularity {
            # The backend custom configuration.
            actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"

            config {
                run-in-background = true

                runtime-attributes = """
                  String? docker
                """

                submit = "/bin/bash ${script}"

                submit-docker = """
                  # Build the Docker image into a singularity image, using the head node.

                  # Make sure  SINGULARITY_CACHEDIR environment variable is set to a location 
                  # on the filesystem that is reachable by the worker nodes!
                  #export SINGULARITY_CACHEDIR=/global/scratch/jaws/singularity_files
                  #export SINGULARITY_TMPDIR=/global/scratch/jaws/singularity_files
                  #export SINGULARITY_LOCALCACHEDIR=/global/scratch/jaws/singularity_files
                  export SINGULARITY_CACHEDIR=/global/scratch/jfroula/singularity_files
                  export SINGULARITY_TMPDIR=/global/scratch/jfroula/singularity_files
                  export SINGULARITY_LOCALCACHEDIR=/global/jfroula/jaws/singularity_files

                  # make sure docker image is pulled.
                  # If we are using a cache we need to ensure that submit processes started by 
                  # Cromwell do not pull the same cache at the same time causing a corrupted cache.
                  # Create an exclusive filelock with flock. 
                  #
                  # Use --containall:  By default, Singularity will mount the user's home directory and 
                  # import the user's environment as well as some other things that make Singularity 
                  # easier to use in an interactive shell. This is bad for reproducibility when there are many users.
                  mkdir -p $SINGULARITY_CACHEDIR  
                  LOCK_FILE=$SINGULARITY_CACHEDIR/singularity_pull_flock

                  flock --exclusive --timeout 900 $LOCK_FILE \
                  singularity exec --containall docker://${docker} \
                  echo "successfully pulled ${docker}!"


                  #DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker})
                  #IMAGE=${cwd}/$DOCKER_NAME.sif
                  #if [ ! -f $IMAGE ]; then
                  #    singularity pull $IMAGE docker://${docker}
                  #fi

                  # validate sif was created
                  #if [ ! -f $IMAGE ]; then
                  #    >&2 echo "Image was not found in docker registry: ${docker}"
                  #    exit 1
                  #fi

                  singularity exec --containall \
                  --bind ${cwd}:${docker_cwd} \
                  --bind /global/scratch/jaws/refdata:/refdata \
                  docker://${docker} ${job_shell} ${script}
                """

                kill = "scancel ${job_id}"
                check-alive = "squeue -j ${job_id}"
                job-id-regex = "Submitted batch job (\\d+).*"

                # Root directory where Cromwell writes job results in the container. This value
                # can be used to specify where the execution folder is mounted in the container.
                # it is used for the construction of the docker_cwd string in the submit-docker
                # value above AND in the generation of the "script" file.
                dockerRoot = /global/scratch/jfroula/JAWS/jaws/examples/leo_dapseq/cromwell-executions
            }
        }
    }
}

