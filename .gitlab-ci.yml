stages:
  - unit-test
  - package
  - deploy-jaws
  - integration-test
  - .post

variables:
  GIT_STRATEGY: clone

# this needs to be commented out in the jobs definitions, for the jobs to
# run in any case and not only when pushing a merge request.
.only-default: &only-default
  only:
    - master
    - branches
    - merge_requests
    - tags

test-rpc:
  <<: *only-default
  stage: unit-test
  script:
    - python3 -m venv test-venv
    - source test-venv/bin/activate
    - pip install -r rpc/requirements.txt
    - pip install flake8
    - pip install pytest
    - make test-rpc
test-site:
  <<: *only-default
  stage: unit-test
  script:
    - python3 -m venv test-venv
    - source test-venv/bin/activate
    - pip install rpc/
    - pip install -r site/requirements.txt
    - pip install flake8
    - pip install pytest
    - make test-site
test-central:
  <<: *only-default
  stage: unit-test
  script:
    - python3 -m venv test-venv
    - source test-venv/bin/activate
    - pip install rpc/
    - pip install -r central/requirements.txt
    - pip install flake8
    - pip install pytest
    - make test-central
test-client:
  <<: *only-default
  stage: unit-test
  script:
    - export PATH=/jaws-ci/bin:$PATH
    - python3 -m venv test-venv
    - source test-venv/bin/activate
    - pip install -r client/requirements.txt
    - pip install flake8
    - pip install pytest
    - make test-client
test-jtm:
  <<: *only-default
  stage: unit-test
  script:
    - python3 -m venv test-venv
    - source test-venv/bin/activate
    - pip install rpc/
    - pip install -r jtm/requirements.txt
    - pip install flake8
    - pip install pytest
    - make test-jtm
package:
  <<: *only-default
  stage: package
  script:
    - python3 -m venv pkg-venv
    - source pkg-venv/bin/activate
    - pip install wheel
    - make pkg
  artifacts:
    name: "packages-$CI_PROJECT_NAME-$CI_PIPELINE_ID"
    expire_in: 1 hour
    paths:
      - rpc/dist
      - site/dist
      - client/dist
      - central/dist
      - jtm/dist
## integration tests - only run on merge request
deploy-jaws-cori-dev:
  stage: deploy-jaws
  tags:
    - cori
  variables:
    DEPLOYMENT_NAME: dev
    INSTALL_DIR: /tmp/jaws-dev
    JAWS_GROUP: jaws
    JTM_GROUP: jaws_jtm
    CLIENT_GROUP: genome
    CLIENT_INSTALL_DIR: /global/cfs/projectdirs/jaws/jaws-dev
    JTM_WORKER_INSTALL_DIR: /global/cfs/projectdirs/jaws_jtm/jtm-dev
    REF_DATA_DIR: /global/cfs/projectdirs/jaws/refdata
    LOG_LEVEL: DEBUG
    SUPERVISOR_DIR: /tmp/jaws-supervisord-dev
    SUPERVISOR_URL: http://cori20.nersc.gov
    JAWS_SUPERVISOR_PORT: 64101
    JTM_SUPERVISOR_PORT: 64111
    JAWS_AUTH_PORT: 3001
    JAWS_REST_PORT: 5001
    CROMWELL_PORT: 50101
    CROMWELL_JAR: /global/cfs/projectdirs/jaws/cromwell/cromwell-52.jar
  script:
    # stop services
    - test -f $SUPERVISOR_DIR/supervisord-jaws.conf && $SUPERVISOR_DIR/bin/supervisorctl -c $SUPERVISOR_DIR/supervisord-jaws.conf stop jaws-$DEPLOYMENT_NAME:*
    - test -f $SUPERVISOR_DIR/supervisord-jtm.conf && $SUPERVISOR_DIR/bin/supervisorctl -c $SUPERVISOR_DIR/supervisord-jtm.conf stop jaws-$DEPLOYMENT_NAME:*
    # setup configs, shims, venvs, and supervisord
    - ./test/integration/cori/generate-configs
    - ./test/integration/cori/generate-shims
    - ./test/integration/cori/generate-venvs
    - ./test/integration/cori/deploy-supervisord
    ## cp cromwell
    - cp $CROMWELL_JAR $INSTALL_DIR/cromwell.jar && chgrp $JTM_GROUP $INSTALL_DIR/cromwell.jar && chmod 640 $INSTALL_DIR/cromwell.jar
    ## start services
    - $SUPERVISOR_DIR/bin/supervisorctl -c $SUPERVISOR_DIR/supervisord-jaws.conf reread
    - $SUPERVISOR_DIR/bin/supervisorctl -c $SUPERVISOR_DIR/supervisord-jaws.conf start jaws-$DEPLOYMENT_NAME:*
    - $SUPERVISOR_DIR/bin/supervisorctl -c $SUPERVISOR_DIR/supervisord-jtm.conf reread
    - $SUPERVISOR_DIR/bin/supervisorctl -c $SUPERVISOR_DIR/supervisord-jtm.conf start jaws-$DEPLOYMENT_NAME:*
    # the following statement would deploy the jtm config file to the parallel fs for use by the jtm worker. problem: doing a chgrp on gpfs does not work through
    # the gitlab-runner. no fix could by found is why that copy now happens in the jtm-worker-deploy shim created by generate-shims. this is a hack.
    # - cp -a $INSTALL_DIR/configs/jaws-jtm.conf $JTM_WORKER_INSTALL_DIR/jaws-jtm.conf && chgrp $JTM_GROUP && chmod 640 $JTM_WORKER_INSTALL_DIR/jaws-jtm.conf
  only:
  - dev
deploy-jaws-cori-staging:
  stage: deploy-jaws
  tags:
    - cori
  variables:
    DEPLOYMENT_NAME: staging
    INSTALL_DIR: /tmp/jaws-staging
    JAWS_GROUP: jaws
    JTM_GROUP: jaws_jtm
    CLIENT_GROUP: genome
    CLIENT_INSTALL_DIR: /global/cfs/projectdirs/jaws/jaws-staging
    JTM_WORKER_INSTALL_DIR: /global/cfs/projectdirs/jaws_jtm/jtm-staging
    REF_DATA_DIR: /global/cfs/projectdirs/jaws/refdata
    LOG_LEVEL: DEBUG
    SUPERVISOR_DIR: /tmp/jaws-supervisord-staging
    SUPERVISOR_URL: http://cori20.nersc.gov
    JAWS_SUPERVISOR_PORT: 64102
    JTM_SUPERVISOR_PORT: 64112
    JAWS_AUTH_PORT: 3002
    JAWS_REST_PORT: 5002
    CROMWELL_PORT: 50102
    CROMWELL_JAR: /global/cfs/projectdirs/jaws/cromwell/cromwell-52.jar
  script:
    # stop services
    - test -f $SUPERVISOR_DIR/supervisord-jaws.conf && $SUPERVISOR_DIR/bin/supervisorctl -c $SUPERVISOR_DIR/supervisord-jaws.conf stop jaws-$DEPLOYMENT_NAME:*
    - test -f $SUPERVISOR_DIR/supervisord-jtm.conf && $SUPERVISOR_DIR/bin/supervisorctl -c $SUPERVISOR_DIR/supervisord-jtm.conf stop jaws-$DEPLOYMENT_NAME:*
    # setup configs, shims, venvs, and supervisord
    - ./test/integration/cori/generate-configs
    - ./test/integration/cori/generate-shims
    - ./test/integration/cori/generate-venvs
    - ./test/integration/cori/deploy-supervisord
    ## cp cromwell
    - cp $CROMWELL_JAR $INSTALL_DIR/cromwell.jar && chgrp $JTM_GROUP $INSTALL_DIR/cromwell.jar && chmod 640 $INSTALL_DIR/cromwell.jar
    ## start services
    - $SUPERVISOR_DIR/bin/supervisorctl -c $SUPERVISOR_DIR/supervisord-jaws.conf reread
    - $SUPERVISOR_DIR/bin/supervisorctl -c $SUPERVISOR_DIR/supervisord-jaws.conf start jaws-$DEPLOYMENT_NAME:*
    - $SUPERVISOR_DIR/bin/supervisorctl -c $SUPERVISOR_DIR/supervisord-jtm.conf reread
    - $SUPERVISOR_DIR/bin/supervisorctl -c $SUPERVISOR_DIR/supervisord-jtm.conf start jaws-$DEPLOYMENT_NAME:*
    # the following statement would deploy the jtm config file to the parallel fs for use by the jtm worker. problem: doing a chgrp on gpfs does not work through
    # the gitlab-runner. no fix could by found is why that copy now happens in the jtm-worker-deploy shim created by generate-shims. this is a hack.
    # - cp -a $INSTALL_DIR/configs/jaws-jtm.conf $JTM_WORKER_INSTALL_DIR/jaws-jtm.conf && chgrp $JTM_GROUP && chmod 640 $JTM_WORKER_INSTALL_DIR/jaws-jtm.conf
  only:
  - staging
deploy-jaws-cori-prod:
  stage: deploy-jaws
  tags:
    - cori
  variables:
    DEPLOYMENT_NAME: prod
    INSTALL_DIR: /tmp/jaws-prod
    JAWS_GROUP: jaws
    JTM_GROUP: jaws_jtm
    CLIENT_GROUP: genome
    CLIENT_INSTALL_DIR: /global/cfs/projectdirs/jaws/jaws-prod
    JTM_WORKER_INSTALL_DIR: /global/cfs/projectdirs/jaws_jtm/jtm-prod
    REF_DATA_DIR: /global/cfs/projectdirs/jaws/refdata
    LOG_LEVEL: DEBUG
    SUPERVISOR_DIR: /tmp/jaws-supervisord-prod
    SUPERVISOR_URL: http://cori20.nersc.gov
    JAWS_SUPERVISOR_PORT: 64103
    JTM_SUPERVISOR_PORT: 64113
    JAWS_AUTH_PORT: 3003
    JAWS_REST_PORT: 5003
    CROMWELL_PORT: 50103
    CROMWELL_JAR: /global/cfs/projectdirs/jaws/cromwell/cromwell-52.jar
  script:
    # stop services
    - test -f $SUPERVISOR_DIR/supervisord-jaws.conf && $SUPERVISOR_DIR/bin/supervisorctl -c $SUPERVISOR_DIR/supervisord-jaws.conf stop jaws-$DEPLOYMENT_NAME:*
    - test -f $SUPERVISOR_DIR/supervisord-jtm.conf && $SUPERVISOR_DIR/bin/supervisorctl -c $SUPERVISOR_DIR/supervisord-jtm.conf stop jaws-$DEPLOYMENT_NAME:*
    # setup configs, shims, venvs, and supervisord
    - ./test/integration/cori/generate-configs
    - ./test/integration/cori/generate-shims
    - ./test/integration/cori/generate-venvs
    - ./test/integration/cori/deploy-supervisord
    ## cp cromwell
    - cp $CROMWELL_JAR $INSTALL_DIR/cromwell.jar && chgrp $JTM_GROUP $INSTALL_DIR/cromwell.jar && chmod 640 $INSTALL_DIR/cromwell.jar
    ## start services
    - $SUPERVISOR_DIR/bin/supervisorctl -c $SUPERVISOR_DIR/supervisord-jaws.conf reread
    - $SUPERVISOR_DIR/bin/supervisorctl -c $SUPERVISOR_DIR/supervisord-jaws.conf start jaws-$DEPLOYMENT_NAME:*
    - $SUPERVISOR_DIR/bin/supervisorctl -c $SUPERVISOR_DIR/supervisord-jtm.conf reread
    - $SUPERVISOR_DIR/bin/supervisorctl -c $SUPERVISOR_DIR/supervisord-jtm.conf start jaws-$DEPLOYMENT_NAME:*
    # the following statement would deploy the jtm config file to the parallel fs for use by the jtm worker. problem: doing a chgrp on gpfs does not work through
    # the gitlab-runner. no fix could by found is why that copy now happens in the jtm-worker-deploy shim created by generate-shims. this is a hack.
    # - cp -a $INSTALL_DIR/configs/jaws-jtm.conf $JTM_WORKER_INSTALL_DIR/jaws-jtm.conf && chgrp $JTM_GROUP && chmod 640 $JTM_WORKER_INSTALL_DIR/jaws-jtm.conf
  only:
  - master
deploy-jaws-lrc-dev:
  stage: deploy-jaws
  tags:
    - lrc
  script:
    ## configs first, to be able to talk to supervisord
    - test -d /tmp/jaws-dev/ || mkdir /tmp/jaws-dev/
    - test -d /tmp/jaws-dev/configs || mkdir /tmp/jaws-dev/configs
    - test -d /tmp/jaws-dev/logs || mkdir /tmp/jaws-dev/logs && test -d /tmp/jaws-dev/logs/jtm || mkdir /tmp/jaws-dev/logs/jtm && chgrp jaws /tmp/jaws-dev/logs/jtm && chmod 770 /tmp/jaws-dev/logs/jtm
    - chgrp jaws /tmp/jaws-dev/ && chmod 770 /tmp/jaws-dev/
    - chgrp jaws /tmp/jaws-dev/configs && chmod 770 /tmp/jaws-dev/configs
    - chgrp jaws /tmp/jaws-dev/logs && chmod 770 /tmp/jaws-dev/logs
    - chgrp jaws /tmp/jaws-dev/logs/jtm && chmod 770 /tmp/jaws-dev/logs/jtm
    - chgrp jaws /tmp/jaws-dev/shims && chmod 770 /tmp/jaws-dev/shims
    - ./test/integration/lrc-dev/generate-configs /tmp/jaws-dev/configs/
    - chmod 660 /tmp/jaws-dev/configs/*
    - chgrp jaws /tmp/jaws-dev/configs/jaws-jtm.conf && chmod 660 /tmp/jaws-dev/configs/jaws-jtm.conf
    - chgrp jaws /tmp/jaws-dev/configs/cromwell.conf && chmod 660 /tmp/jaws-dev/configs/cromwell.conf
    - module load python
    - ./test/integration/lrc-dev/deploy-supervisord /tmp/jaws-supervisord-dev/ /tmp/jaws-dev/shims/
    - chgrp jaws /tmp/jaws-supervisord-dev/ && chgrp -R jaws /tmp/jaws-supervisord-dev/{bin,venv}
    - ./test/integration/lrc-dev/generate-shims /tmp/jaws-dev/
    - chgrp jaws /tmp/jaws-dev/shims/jaws-jtm-dev && chmod 770 /tmp/jaws-dev/shims/jaws-jtm-dev
    - chgrp jaws /tmp/jaws-dev/shims/jaws-cromwell-dev && chmod 770 /tmp/jaws-dev/shims/jaws-cromwell-dev
    - make pkg
    - /tmp/jaws-supervisord-dev/bin/supervisorctl -c /tmp/jaws-supervisord-dev/supervisord-jaws.conf stop jaws-dev:*
    - rm -rf /tmp/jaws-dev/site && python -m venv /tmp/jaws-dev/site    && . /tmp/jaws-dev/site/bin/activate && pip install rpc/dist/*    && pip install site/dist/*    && deactivate
    - rm -rf /tmp/jaws-dev/jtm && python -m venv /tmp/jaws-dev/jtm     && . /tmp/jaws-dev/jtm/bin/activate && pip install rpc/dist/* && pip install jtm/dist/*     && deactivate && chmod -R g+rx,o+rx /tmp/jaws-dev/jtm || true
    - cp /global/home/groups-sw/lr_jgicloud/cromwell/cromwell.jar /tmp/jaws-dev/cromwell.jar && chgrp jaws /tmp/jaws-dev/cromwell.jar
    - rm -rf /global/home/groups-sw/lr_jgicloud/jtm-dev && python -m venv /global/home/groups-sw/lr_jgicloud/jtm-dev && . /global/home/groups-sw/lr_jgicloud/jtm-dev/bin/activate && pip install rpc/dist/* && pip install jtm/dist/* && deactivate && chmod -R g+rx,o+rx /global/home/groups-sw/lr_jgicloud/jtm-dev || true
    - chgrp jaws /tmp/jaws-dev/configs/jaws-jtm.conf && chmod 660 /tmp/jaws-dev/configs/jaws-jtm.conf
    - /tmp/jaws-supervisord-dev/bin/supervisorctl -c /tmp/jaws-supervisord-dev/supervisord-jaws.conf stop jaws-dev:*
    - /tmp/jaws-supervisord-dev/bin/supervisorctl -c /tmp/jaws-supervisord-dev/supervisord-jaws.conf reread
    - /tmp/jaws-supervisord-dev/bin/supervisorctl -c /tmp/jaws-supervisord-dev/supervisord-jaws.conf start jaws-dev:*
    # the following statement would deploy the jtm config file to the parallel fs for use by the jtm worker. problem: doing a chgrp on gpfs does not work through
    # the gitlab-runner. no fix could by found is why that copy now happens in the jtm-manager shim created by generate-shims. this is a hack.
    # - cp -a /tmp/jaws-dev/configs/jaws-jtm.conf /global/home/groups-sw/lr_jgicloud/jtm-dev/jaws-jtm.conf
  only:
  - dev

deploy-jaws-lrc-staging:
  stage: deploy-jaws
  tags:
    - lrc
  script:
    ## configs first, to be able to talk to supervisord
    - test -d /tmp/jaws-staging/ || mkdir /tmp/jaws-staging/
    - test -d /tmp/jaws-staging/configs || mkdir /tmp/jaws-staging/configs
    - test -d /tmp/jaws-staging/logs || mkdir /tmp/jaws-staging/logs && test -d /tmp/jaws-staging/logs/jtm || mkdir /tmp/jaws-staging/logs/jtm && chgrp jaws /tmp/jaws-staging/logs/jtm && chmod 770 /tmp/jaws-staging/logs/jtm
    - chgrp jaws /tmp/jaws-staging/ && chmod 770 /tmp/jaws-staging/
    - chgrp jaws /tmp/jaws-staging/configs && chmod 770 /tmp/jaws-staging/configs
    - chgrp jaws /tmp/jaws-staging/logs && chmod 770 /tmp/jaws-staging/logs
    - chgrp jaws /tmp/jaws-staging/logs/jtm && chmod 770 /tmp/jaws-staging/logs/jtm
    - chgrp jaws /tmp/jaws-staging/shims && chmod 770 /tmp/jaws-staging/shims
    - ./test/integration/lrc-staging/generate-configs /tmp/jaws-staging/configs/
    - chmod 660 /tmp/jaws-staging/configs/*
    - chgrp jaws /tmp/jaws-staging/configs/jaws-jtm.conf && chmod 660 /tmp/jaws-staging/configs/jaws-jtm.conf
    - chgrp jaws /tmp/jaws-staging/configs/cromwell.conf && chmod 660 /tmp/jaws-staging/configs/cromwell.conf
    - module load python
    - ./test/integration/lrc-staging/deploy-supervisord /tmp/jaws-supervisord-staging/ /tmp/jaws-staging/shims/
    - chgrp jaws /tmp/jaws-supervisord-staging/ && chgrp -R jaws /tmp/jaws-supervisord-staging/{bin,venv}
    - ./test/integration/lrc-staging/generate-shims /tmp/jaws-staging/
    - chgrp jaws /tmp/jaws-staging/shims/jaws-jtm-staging && chmod 770 /tmp/jaws-staging/shims/jaws-jtm-staging
    - chgrp jaws /tmp/jaws-staging/shims/jaws-cromwell-staging && chmod 770 /tmp/jaws-staging/shims/jaws-cromwell-staging
    - make pkg
    - /tmp/jaws-supervisord-staging/bin/supervisorctl -c /tmp/jaws-supervisord-staging/supervisord-jaws.conf stop jaws-staging:*
    - rm -rf /tmp/jaws-staging/site && python -m venv /tmp/jaws-staging/site    && . /tmp/jaws-staging/site/bin/activate && pip install rpc/dist/*    && pip install site/dist/*    && deactivate
    - rm -rf /tmp/jaws-staging/jtm && python -m venv /tmp/jaws-staging/jtm     && . /tmp/jaws-staging/jtm/bin/activate && pip install rpc/dist/* && pip install jtm/dist/*     && deactivate && chmod -R g+rx,o+rx /tmp/jaws-staging/jtm || true
    - cp /global/home/groups-sw/lr_jgicloud/cromwell/cromwell.jar /tmp/jaws-staging/cromwell.jar && chgrp jaws /tmp/jaws-staging/cromwell.jar
    - rm -rf /global/home/groups-sw/lr_jgicloud/jtm-staging && python -m venv /global/home/groups-sw/lr_jgicloud/jtm-staging && . /global/home/groups-sw/lr_jgicloud/jtm-staging/bin/activate && pip install rpc/dist/* && pip install jtm/dist/* && deactivate && chmod -R g+rx,o+rx /global/home/groups-sw/lr_jgicloud/jtm-staging || true
    - chgrp jaws /tmp/jaws-staging/configs/jaws-jtm.conf && chmod 660 /tmp/jaws-staging/configs/jaws-jtm.conf
    - /tmp/jaws-supervisord-staging/bin/supervisorctl -c /tmp/jaws-supervisord-staging/supervisord-jaws.conf stop jaws-staging:*
    - /tmp/jaws-supervisord-staging/bin/supervisorctl -c /tmp/jaws-supervisord-staging/supervisord-jaws.conf reread
    - /tmp/jaws-supervisord-staging/bin/supervisorctl -c /tmp/jaws-supervisord-staging/supervisord-jaws.conf start jaws-staging:*
    # the following statement would deploy the jtm config file to the parallel fs for use by the jtm worker. problem: doing a chgrp on gpfs does not work through
    # the gitlab-runner. no fix could by found is why that copy now happens in the jtm-manager shim created by generate-shims. this is a hack.
    # - cp -a /tmp/jaws-staging/configs/jaws-jtm.conf /global/home/groups-sw/lr_jgicloud/jtm-staging/jaws-jtm.conf
  only:
  - staging

deploy-jaws-lrc-prod:
  stage: deploy-jaws
  tags:
    - lrc
  script:
    ## configs first, to be able to talk to supervisord
    - test -d /tmp/jaws-prod/ || mkdir /tmp/jaws-prod/
    - test -d /tmp/jaws-prod/configs || mkdir /tmp/jaws-prod/configs
    - test -d /tmp/jaws-prod/logs || mkdir /tmp/jaws-prod/logs && test -d /tmp/jaws-prod/logs/jtm || mkdir /tmp/jaws-prod/logs/jtm && chgrp jaws /tmp/jaws-prod/logs/jtm && chmod 770 /tmp/jaws-prod/logs/jtm
    - chgrp jaws /tmp/jaws-prod/ && chmod 770 /tmp/jaws-prod/
    - chgrp jaws /tmp/jaws-prod/configs && chmod 770 /tmp/jaws-prod/configs
    - chgrp jaws /tmp/jaws-prod/logs && chmod 770 /tmp/jaws-prod/logs
    - chgrp jaws /tmp/jaws-prod/logs/jtm && chmod 770 /tmp/jaws-prod/logs/jtm
    - chgrp jaws /tmp/jaws-prod/shims && chmod 770 /tmp/jaws-prod/shims
    - ./test/integration/lrc-prod/generate-configs /tmp/jaws-prod/configs/
    - chmod 660 /tmp/jaws-prod/configs/*
    - chgrp jaws /tmp/jaws-prod/configs/jaws-jtm.conf && chmod 660 /tmp/jaws-prod/configs/jaws-jtm.conf
    - chgrp jaws /tmp/jaws-prod/configs/cromwell.conf && chmod 660 /tmp/jaws-prod/configs/cromwell.conf
    - module load python
    - ./test/integration/lrc-prod/deploy-supervisord /tmp/jaws-supervisord-prod/ /tmp/jaws-prod/shims/
    - chgrp jaws /tmp/jaws-supervisord-prod/ && chgrp -R jaws /tmp/jaws-supervisord-prod/{bin,venv}
    - ./test/integration/lrc-prod/generate-shims /tmp/jaws-prod/
    - chgrp jaws /tmp/jaws-prod/shims/jaws-jtm-prod && chmod 770 /tmp/jaws-prod/shims/jaws-jtm-prod
    - chgrp jaws /tmp/jaws-prod/shims/jaws-cromwell-prod && chmod 770 /tmp/jaws-prod/shims/jaws-cromwell-prod
    - make pkg
    - /tmp/jaws-supervisord-prod/bin/supervisorctl -c /tmp/jaws-supervisord-prod/supervisord-jaws.conf stop jaws-prod:*
    - rm -rf /tmp/jaws-prod/site && python -m venv /tmp/jaws-prod/site    && . /tmp/jaws-prod/site/bin/activate && pip install rpc/dist/*    && pip install site/dist/*    && deactivate
    - rm -rf /tmp/jaws-prod/jtm && python -m venv /tmp/jaws-prod/jtm     && . /tmp/jaws-prod/jtm/bin/activate && pip install rpc/dist/* && pip install jtm/dist/*     && deactivate && chmod -R g+rx,o+rx /tmp/jaws-prod/jtm || true
    - cp /global/home/groups-sw/lr_jgicloud/cromwell/cromwell.jar /tmp/jaws-prod/cromwell.jar && chgrp jaws /tmp/jaws-prod/cromwell.jar
    - rm -rf /global/home/groups-sw/lr_jgicloud/jtm-prod && python -m venv /global/home/groups-sw/lr_jgicloud/jtm-prod && . /global/home/groups-sw/lr_jgicloud/jtm-prod/bin/activate && pip install rpc/dist/* && pip install jtm/dist/* && deactivate && chmod -R g+rx,o+rx /global/home/groups-sw/lr_jgicloud/jtm-prod || true
    - chgrp jaws /tmp/jaws-prod/configs/jaws-jtm.conf && chmod 660 /tmp/jaws-prod/configs/jaws-jtm.conf
    - /tmp/jaws-supervisord-prod/bin/supervisorctl -c /tmp/jaws-supervisord-prod/supervisord-jaws.conf stop jaws-prod:*
    - /tmp/jaws-supervisord-prod/bin/supervisorctl -c /tmp/jaws-supervisord-prod/supervisord-jaws.conf reread
    - /tmp/jaws-supervisord-prod/bin/supervisorctl -c /tmp/jaws-supervisord-prod/supervisord-jaws.conf start jaws-prod:*
    # the following statement would deploy the jtm config file to the parallel fs for use by the jtm worker. problem: doing a chgrp on gpfs does not work through
    # the gitlab-runner. no fix could by found is why that copy now happens in the jtm-manager shim created by generate-shims. this is a hack.
    # - cp -a /tmp/jaws-prod/configs/jaws-jtm.conf /global/home/groups-sw/lr_jgicloud/jtm-prod/jaws-jtm.conf
  only:
  - master

deploy-jaws-cascade:
  stage: deploy-jaws
  tags:
    - cascade
  script:
    ## configs first, to be able to talk to supervisord
    - export LC_ALL=en_US.UTF-8
    - export LANG=en_US.UTF-8
    - export PYTHONIOENCODING=utf-8
    - source /opt/rh/rh-python36/enable
    - test -d /tmp/jaws-dev/ || mkdir /tmp/jaws-dev/
    - test -d /tmp/jaws-dev/configs || mkdir /tmp/jaws-dev/configs
    - test -d /tmp/jaws-dev/logs || mkdir /tmp/jaws-dev/logs && test -d /tmp/jaws-dev/logs/jtm || mkdir /tmp/jaws-dev/logs/jtm && chgrp svc-jtm-user /tmp/jaws-dev/logs/jtm && chmod 770 /tmp/jaws-dev/logs/jtm
    - chgrp svc-jtm-user /tmp/jaws-dev/ && chmod 770 /tmp/jaws-dev/
    - chgrp svc-jtm-user /tmp/jaws-dev/configs && chmod 770 /tmp/jaws-dev/configs
    - chgrp svc-jtm-user /tmp/jaws-dev/logs && chmod 770 /tmp/jaws-dev/logs
    - chgrp svc-jtm-user /tmp/jaws-dev/logs/jtm && chmod 770 /tmp/jaws-dev/logs/jtm
    - chgrp svc-jtm-user /tmp/jaws-dev/shims && chmod 770 /tmp/jaws-dev/shims
    - ./test/integration/pnnl/generate-configs /tmp/jaws-dev/configs/
    - chmod 660 /tmp/jaws-dev/configs/*
    - chgrp svc-jtm-user /tmp/jaws-dev/configs/jaws-jtm.conf && chmod 660 /tmp/jaws-dev/configs/jaws-jtm.conf
    - chgrp svc-jtm-user /tmp/jaws-dev/configs/cromwell.conf && chmod 660 /tmp/jaws-dev/configs/cromwell.conf
    - chgrp svc-jtm-user /tmp/jaws-supervisord/supervisord-jtm.conf && chmod 660 /tmp/jaws-supervisord/supervisord-jtm.conf
    - ./test/integration/pnnl/deploy-supervisord /tmp/jaws-supervisord/ /tmp/jaws-dev/shims/
    - chgrp svc-jtm-user /tmp/jaws-supervisord/ && chgrp -R svc-jtm-user /tmp/jaws-supervisord/{bin,venv}
    - ./test/integration/pnnl/generate-shims /tmp/jaws-dev/
    - chgrp svc-jtm-user /tmp/jaws-dev/shims/jaws-jtm-dev && chmod 770 /tmp/jaws-dev/shims/jaws-jtm-dev
    - chgrp svc-jtm-user /tmp/jaws-dev/shims/jaws-cromwell-dev && chmod 770 /tmp/jaws-dev/shims/jaws-cromwell-dev
    - make pkg
    ## stop jaws and install new versions
    - /tmp/jaws-supervisord/bin/supervisorctl -c /tmp/jaws-supervisord/supervisord-jaws.conf stop jaws-dev:*
    - rm -rf /tmp/jaws-dev/client && python3.6 -m venv /tmp/jaws-dev/client  && . /tmp/jaws-dev/client/bin/activate  && pip3 install client/dist/*  && deactivate
    - rm -rf /tmp/jaws-dev/site && python3.6 -m venv /tmp/jaws-dev/site    && . /tmp/jaws-dev/site/bin/activate    && pip3 install rpc/dist/* && pip install site/dist/*    && deactivate
    - /tmp/jaws-supervisord/bin/supervisorctl -c /tmp/jaws-supervisord/supervisord-jaws.conf reread
    ## start jaws
    - /tmp/jaws-supervisord/bin/supervisorctl -c /tmp/jaws-supervisord/supervisord-jaws.conf start jaws-dev:*
    ## stop jtm and cromwell and install new versions
    - rm -rf /tmp/jaws-dev/jtm && python3.6 -m venv /tmp/jaws-dev/jtm     && . /tmp/jaws-dev/jtm/bin/activate     && pip3 install rpc/dist/* && pip install jtm/dist/*     && deactivate && chmod -R g+rx,o+rx /tmp/jaws-dev/jtm || true
    - wget -q -O /tmp/jaws-dev/cromwell.jar https://github.com/broadinstitute/cromwell/releases/download/52/cromwell-52.jar && chgrp svc-jtm-user /tmp/jaws-dev/cromwell.jar
    - /tmp/jaws-supervisord/bin/supervisorctl -c /tmp/jaws-supervisord/supervisord-jtm.conf stop jaws-dev:*
    - /tmp/jaws-supervisord/bin/supervisorctl -c /tmp/jaws-supervisord/supervisord-jtm.conf reread
    - /tmp/jaws-supervisord/bin/supervisorctl -c /tmp/jaws-supervisord/supervisord-jtm.conf start jaws-dev:*
    ## start jtm
    - mkdir -p /dtemp/mscjgi/jaws-dev/jtm || true
    - rm -rf /dtemp/mscjgi/jaws-dev/jtm && python3.6 -m venv /dtemp/mscjgi/jaws-dev/jtm && . /dtemp/mscjgi/jaws-dev/jtm/bin/activate && pip3 install rpc/dist/* && pip install jtm/dist/* && deactivate && chmod -R g+rx,o+rx /dtemp/mscjgi/jaws-dev/jtm || true
    - chgrp svc-jtm-user /tmp/jaws-dev/configs/jaws-jtm.conf && chmod 660 /tmp/jaws-dev/configs/jaws-jtm.conf
    # the following statement would deploy the jtm config file to the parallel fs for use by the jtm worker. problem: doing a chgrp on gpfs does not work through
    # the gitlab-runner. no fix could by found is why that copy now happens in the jtm-manager shim created by generate-shims. this is a hack.
    # - cp -a /tmp/jaws-dev/configs/jaws-jtm.conf /global/cfs/projectdirs/jaws/jtm-dev/jaws-jtm.conf
