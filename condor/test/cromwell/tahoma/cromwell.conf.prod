include required(classpath("application"))
webservice
{
  port = 50103
  interface = 127.0.0.1
}
system
{
  abort-jobs-on-terminate = true
  graceful-server-shutdown = true
  workflow-restart = false
  max-concurrent-workflows = 100000
  max-workflow-launch-count = 100000
  new-workflow-poll-rate = 1
  number-of-workflow-log-copy-workers = 20
  number-of-cache-read-workers = 50
  job-rate-control
  {
    jobs = 1
    per = 1 second
  }
}
workflow-options
{
  workflow-log-dir: "/tahoma/mscjgi/jaws-install/jaws-prod/logs/cromwell-workflow-logs"
  workflow-log-temporary: false
  workflow-failure-mode: "ContinueWhilePossible"
  default
  {
    workflow-type: WDL
    workflow-type-version: "draft-2"
  }
}
call-caching
{
  enabled = true
  invalidate-bad-cache-result = true
}
# this is required for shifter to find image from its registry.
docker {
    hash-lookup {
        enabled = false
    }
}
backend
{
  default = "HtCondor"
  providers
  {
    LOCAL
    {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        concurrent-job-limit = 0
      }
    }
    JTM
    {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        exit-code-timeout-seconds = 60
        runtime-attributes = """
          String? docker
          String time = "00:30:00"
          Int cpu = 32
          Float? memory_gb = 5
          String cluster = "tahoma"
          String poolname = "small"
          String constraint = ""
          String qos = ""
          String account = "mscjgi"
          String partition = ""
          Int node = 1
          Int nwpn = 1
          Int shared = 0
        """

        filesystems {
          local {
            localization: [ "hard-link", "copy" ]
            caching {
              duplication-strategy: [ "hard-link", "copy" ]
              hashing-strategy: "xxh64"
            }
            http {}
          }
        }

        kill = "jtm --config=/tahoma/mscjgi/jaws-install/jaws-prod/configs/jaws-jtm.conf kill -tid ${job_id}"
        check-alive = "jtm --config=/tahoma/mscjgi/jaws-install/jaws-prod/configs/jaws-jtm.conf isalive -tid ${job_id}"
        job-id-regex = "JTM task ID (\\d+)"

        submit = """
          CONSTRAINT=${constraint}
          QOS=${qos}
          PARTITION=${partition}
          [ -z $QOS ] || QOS="--qos $QOS"
          [ -z $CONSTRAINT ] || CONSTRAINT="-C $CONSTRAINT"
          [ -z $PARTITION ] || PARTITION="-P $PARTITION"
          jtm --config=/tahoma/mscjgi/jaws-install/jaws-prod/configs/jaws-jtm.conf           submit           -cmd '/bin/bash ${script}'           -cl ${cluster}           -t ${time}           -c ${cpu}           -m ${memory_gb}           -p ${poolname}           -N ${node}           -nwpn ${nwpn}           -jid ${job_name}           -A ${account}           --shared ${shared}           $CONSTRAINT           $QOS           $PARTITION
        """

        submit-docker = """
          export SINGULARITY_CACHEDIR=/big_scratch/singularity_files
    export SINGULARITY_PULLFOLDER=/tahoma/mscjgi/singularity_files
    export SINGULARITY_TMPDIR=/big_scratch/singularity_files
    export SINGULARITY_LOCALCACHEDIR=/big_scratch/singularity_files
    export FLOCK_DIR=/tmp

    IMAGE=$(echo ${docker} | tr '/:' '_').sif
    echo "$IMAGE"
    if [ -z $SINGULARITY_CACHEDIR ];
        then CACHE_DIR=$HOME/.singularity/cache
        else CACHE_DIR=$SINGULARITY_CACHEDIR
    fi
    # Make sure cache dir exists so lock file can be created by flock
    mkdir -p $CACHE_DIR
    LOCK_FILE=$FLOCK_DIR/singularity_pull_flock
    out=$(flock --exclusive --timeout 900 $LOCK_FILE     singularity pull $IMAGE docker://${docker}  2>&1)
    ret=$?
    if [[ $ret == 0 ]]; then
        echo "Successfully pulled ${docker}!"
    else
        if [[ $(echo $out | grep "exists" ) ]]; then
            echo "Image file already exists, ${docker}!"
        else
            echo "Failed to pull ${docker}!" >&2
            exit $ret
        fi
    fi

          CONSTRAINT=${constraint}
          QOS=${qos}
          PARTITION=${partition}
          [ -z $QOS ] || QOS="--qos $QOS"
          [ -z $CONSTRAINT ] || CONSTRAINT="-C $CONSTRAINT"
          [ -z $PARTITION ] || PARTITION="-P $PARTITION"
          jtm --config=/tahoma/mscjgi/jaws-install/jaws-prod/configs/jaws-jtm.conf           submit           -cmd "/tahoma/mscjgi/jaws-install/jaws-prod/singularity_exec.sh /tahoma/mscjgi/refdata /refdata ${cwd} ${docker_cwd} $SINGULARITY_PULLFOLDER/$IMAGE ${job_shell} ${script}"           -cl ${cluster}           -t ${time}           -c ${cpu}           -m ${memory_gb}           -p ${poolname}           -N ${node}           -nwpn ${nwpn}           -jid ${job_name}           -A ${account}           --shared ${shared}           $QOS           $CONSTRAINT           $PARTITION
        """

        # Root directory where Cromwell writes job results in the container. This value
        # can be used to specify where the execution folder is mounted in the container.
        # it is used for the construction of the docker_cwd string in the submit-docker
        # value above AND in the generation of the "script" file.
        dockerRoot = /tahoma/mscjgi/jaws-prod/cromwell-executions
      }
    }
    HtCondor {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"

      config
      {
        #exit-code-timeout-seconds = 60
        runtime-attributes = """
          String? docker
          String? docker_user
          Int? cpu = 1
          Float? memory_mb = 512
          Float? disk_kb = 25600.0
          Int? runtime_minutes
          String? nativeSpecs
          String? priority
        """
        submit = """
          export CONDOR_CONFIG="/tahoma/mscjgi/condor/condor_central.config"
          PATH="/tahoma/mscjgi/condor/condor/bin":"/tahoma/mscjgi/condor/condor/sbin":/cluster/apps/python/Python-3.8.1/build/bin:/opt/xcat/bin:/opt/xcat/sbin:/msc/apps/compilers/IPS_2018/compilers_and_libraries_2018.0.128/linux/bin/intel64:/msc/apps/compilers/IPS_2018/compilers_and_libraries_2018.0.128/linux/mpi/intel64/bin:/msc/bin:/cluster/bin:/home/scicons/tahoma/bin:/cluster/apps/spack/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/hpss/bin:/home/svc-jtm-manager/.local/bin:/home/svc-jtm-manager/bin
          chmod 755 ${script}
          #printenv > env.debug
          cat > ${cwd}/execution/submitFile <<EOF
PATH="/tahoma/mscjgi/condor/condor/bin":"/tahoma/mscjgi/condor/condor/sbin":/cluster/apps/python/Python-3.8.1/build/bin:/opt/xcat/bin:/opt/xcat/sbin:/msc/apps/compilers/IPS_2018/compilers_and_libraries_2018.0.128/linux/bin/intel64:/msc/apps/compilers/IPS_2018/compilers_and_libraries_2018.0.128/linux/mpi/intel64/bin:/msc/bin:/cluster/bin:/home/scicons/tahoma/bin:/cluster/apps/spack/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/hpss/bin:/home/svc-jtm-manager/.local/bin:/home/svc-jtm-manager/bin
Iwd=${cwd}/execution
+Owner=UNDEFINED
request_memory=${memory_mb}
request_disk=${disk_kb}
request_cpus=${cpu}
error=${cwd}/execution/stderr
output=${cwd}/execution/stdout
log_xml=true
executable=${script}
log=${cwd}/execution/execution.log
queue
EOF
          condor_submit ${cwd}/execution/submitFile
        """
        submit-docker = """
          export CONDOR_CONFIG="/tahoma/mscjgi/condor/condor_central.config"
          PATH="/tahoma/mscjgi/condor/condor/bin":"/tahoma/mscjgi/condor/condor/sbin":/cluster/apps/python/Python-3.8.1/build/bin:/opt/xcat/bin:/opt/xcat/sbin:/msc/apps/compilers/IPS_2018/compilers_and_libraries_2018.0.128/linux/bin/intel64:/msc/apps/compilers/IPS_2018/compilers_and_libraries_2018.0.128/linux/mpi/intel64/bin:/msc/bin:/cluster/bin:/home/scicons/tahoma/bin:/cluster/apps/spack/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/hpss/bin:/home/svc-jtm-manager/.local/bin:/home/svc-jtm-manager/bin
          export SINGULARITY_CACHEDIR=/big_scratch/singularity_files
    export SINGULARITY_PULLFOLDER=/tahoma/mscjgi/singularity_files
    export SINGULARITY_TMPDIR=/big_scratch/singularity_files
    export SINGULARITY_LOCALCACHEDIR=/big_scratch/singularity_files
    export FLOCK_DIR=/tmp

    IMAGE=$(echo ${docker} | tr '/:' '_').sif
    echo "$IMAGE"
    if [ -z $SINGULARITY_CACHEDIR ];
        then CACHE_DIR=$HOME/.singularity/cache
        else CACHE_DIR=$SINGULARITY_CACHEDIR
    fi
    # Make sure cache dir exists so lock file can be created by flock
    mkdir -p $CACHE_DIR
    LOCK_FILE=$FLOCK_DIR/singularity_pull_flock
    out=$(flock --exclusive --timeout 900 $LOCK_FILE     singularity pull $IMAGE docker://${docker}  2>&1)
    ret=$?
    if [[ $ret == 0 ]]; then
        echo "Successfully pulled ${docker}!"
    else
        if [[ $(echo $out | grep "exists" ) ]]; then
            echo "Image file already exists, ${docker}!"
        else
            echo "Failed to pull ${docker}!" >&2
            exit $ret
        fi
    fi

          #printenv > env.debug
          cat > ${cwd}/execution/dockerScript <<EOF
#!/bin/bash
PATH="/tahoma/mscjgi/condor/condor/bin":"/tahoma/mscjgi/condor/condor/sbin":/cluster/apps/python/Python-3.8.1/build/bin:/opt/xcat/bin:/opt/xcat/sbin:/msc/apps/compilers/IPS_2018/compilers_and_libraries_2018.0.128/linux/bin/intel64:/msc/apps/compilers/IPS_2018/compilers_and_libraries_2018.0.128/linux/mpi/intel64/bin:/msc/bin:/cluster/bin:/home/scicons/tahoma/bin:/cluster/apps/spack/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/hpss/bin:/home/svc-jtm-manager/.local/bin:/home/svc-jtm-manager/bin
/tahoma/mscjgi/jaws-install/jaws-prod/singularity_exec.sh /tahoma/mscjgi/refdata /refdata ${cwd} ${docker_cwd} $SINGULARITY_PULLFOLDER/$IMAGE ${job_shell} ${script}
EOF
          chmod 755 ${cwd}/execution/dockerScript
          cat > ${cwd}/execution/submitFile <<EOF
Iwd=${cwd}/execution
+Owner=UNDEFINED
request_memory=${memory_mb}
request_disk=${disk_kb}
request_cpus=${cpu}
error=${cwd}/execution/stderr
output=${cwd}/execution/stdout
log_xml=true
executable=${cwd}/execution/dockerScript
log=${cwd}/execution/execution.log
queue
EOF
          condor_submit ${cwd}/execution/submitFile
       """
        kill = "export CONDOR_CONFIG=/tahoma/mscjgi/condor/condor_central.config && /tahoma/mscjgi/condor/condor/bin/condor_rm ${job_id}"
        check-alive = "export CONDOR_CONFIG=/tahoma/mscjgi/condor/condor_central.config && /tahoma/mscjgi/condor/condor/bin/condor_q ${job_id}"
        job-id-regex = "(?sm).*cluster (\\d+)..*"
        root = /tahoma/mscjgi/scratch/jaws-prod/cromwell-executions
        dockerRoot = /tahoma/mscjgi/scratch/jaws-prod/cromwell-executions
      }
    }
  }
}
database
{
  profile = "slick.jdbc.MySQLProfile$"
  db
  {
    driver = "com.mysql.cj.jdbc.Driver"
    url = "jdbc:mysql://jaws-db.lbl.gov:3306/cromwell_tahoma_prod?rewriteBatchedStatements=true&useSSL=false&autoReconnect=true&useUnicode=true&useJDBCCompliantTimezoneShift=true&useLegacyDatetimeCode=false&serverTimezone=UTC"
    user = "jaws"
    password = "MBQG0!7jAxk1M2R6Ufh"
    connectionTimeout = 5000
  }
  insert-batch-size = 2000
}
